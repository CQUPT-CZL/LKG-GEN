import os
import json
from collections import defaultdict
from . import config
from .utils import call_llm, load_json, save_json, load_prompt

def create_chunk_to_entities_map(disambiguated_entities: list):
    """
    åˆ›å»ºä¸€ä¸ªä» chunk_id åˆ°å®ä½“åç§°åˆ—è¡¨çš„åå‘ç´¢å¼•ã€‚
    ç°åœ¨chunk_idæ ¼å¼ä¸ºï¼šæ–‡ä»¶å_chunkç¼–å·
    """
    chunk_map = defaultdict(list)
    for entity in disambiguated_entities:
        # æˆ‘ä»¬åªå…³å¿ƒè§„èŒƒåç§°
        canonical_name = entity["entity_text"]
        for chunk_id in entity["chunk_id"]:
            if canonical_name not in chunk_map[chunk_id]:
                chunk_map[chunk_id].append(canonical_name)
    return chunk_map

def load_all_chunks():
    """
    åŠ è½½æ‰€æœ‰chunkæ–‡ä»¶çš„å†…å®¹ï¼Œè¿”å›ä¸€ä¸ªç»Ÿä¸€çš„å­—å…¸
    æ ¼å¼ï¼š{"æ–‡ä»¶å_chunkç¼–å·": "chunkå†…å®¹"}
    """
    all_chunks = {}
    
    for filename in os.listdir(config.CHUNK_OUTPUT_DIR):
        if filename.endswith(".json"):
            chunk_path = os.path.join(config.CHUNK_OUTPUT_DIR, filename)
            file_prefix = filename.replace('.json', '')
            
            chunks = load_json(chunk_path)
            if chunks:
                # ä¸ºæ¯ä¸ªchunk_idæ·»åŠ æ–‡ä»¶åå‰ç¼€
                for chunk_id, chunk_text in chunks.items():
                    prefixed_chunk_id = f"{file_prefix}_{chunk_id}"
                    all_chunks[prefixed_chunk_id] = chunk_text
                    
                print(f"ğŸ“„ åŠ è½½äº†æ–‡ä»¶ {filename}ï¼ŒåŒ…å« {len(chunks)} ä¸ªchunks")
    
    print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(all_chunks)} ä¸ªchunks")
    return all_chunks

def extract_relations_for_chunk(chunk_id: str, chunk_text: str, entities_in_chunk: list):
    """
    ä¸ºå•ä¸ªæ–‡æœ¬å—è°ƒç”¨LLMè¿›è¡Œå…³ç³»æŠ½å–ã€‚
    """
    print(f"ğŸ”„ æ­£åœ¨å¤„ç† Chunk {chunk_id}")
    if len(entities_in_chunk) < 2:
        print("âš ï¸ å®ä½“æ•°é‡å°‘äº2ä¸ªï¼Œæ— æ³•å½¢æˆå…³ç³»ï¼Œè·³è¿‡ã€‚")
        return []

    prompt_template = load_prompt(config.RE_PROMPT_PATH)
    
    # å¡«å……Prompt
    prompt = prompt_template.replace("{{CHUNK_TEXT}}", chunk_text)
    prompt = prompt.replace("{{ENTITIES_IN_CHUNK}}", json.dumps(entities_in_chunk, ensure_ascii=False))
    prompt = prompt.replace("{{RELATION_TYPES}}", json.dumps(config.RELATION_TYPES, ensure_ascii=False))
    
    # è°ƒç”¨LLM
    triples = call_llm(prompt, model_name="qwen-plus-latest")
    
    if triples and isinstance(triples, list):
        print(f"âœ… æˆåŠŸæŠ½å–åˆ° {len(triples)} ä¸ªå…³ç³»ä¸‰å…ƒç»„ã€‚")
        return triples
    else:
        print("âš ï¸ æœªæŠ½å–åˆ°å…³ç³»æˆ–LLMè¿”å›æ ¼å¼é”™è¯¯ã€‚")
        return []

def run_relation_extraction_on_all():
    """
    å¯¹æ‰€æœ‰æ¶ˆæ­§åçš„å®ä½“è¿›è¡Œå…³ç³»æŠ½å–ï¼Œç”Ÿæˆç»Ÿä¸€çš„å…³ç³»æ–‡ä»¶
    """
    print("ğŸš€ å¼€å§‹å…³ç³»æŠ½å–å¤„ç†...")
    
    # åŠ è½½æ¶ˆæ­§åçš„å®ä½“æ–‡ä»¶
    disambiguated_entities_path = os.path.join(config.NER_PRO_OUTPUT_DIR, "all_entities_disambiguated.json")
    
    if not os.path.exists(disambiguated_entities_path):
        print(f"âŒ æ‰¾ä¸åˆ°æ¶ˆæ­§å®ä½“æ–‡ä»¶: {disambiguated_entities_path}")
        return None
    
    print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ¶ˆæ­§å®ä½“æ–‡ä»¶: all_entities_disambiguated.json")
    disambiguated_entities = load_json(disambiguated_entities_path)
    
    if not disambiguated_entities:
        print("âš ï¸ æ¶ˆæ­§å®ä½“æ•°æ®ä¸ºç©º")
        return None
    
    print(f"âœ… åŠ è½½äº† {len(disambiguated_entities)} ä¸ªæ¶ˆæ­§åçš„å®ä½“")
    
    # åŠ è½½æ‰€æœ‰chunkæ–‡ä»¶çš„å†…å®¹
    print("ğŸ“š æ­£åœ¨åŠ è½½æ‰€æœ‰chunkæ–‡ä»¶...")
    all_chunks = load_all_chunks()
    
    if not all_chunks:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•chunkæ•°æ®")
        return None
    
    # åˆ›å»º chunk_id -> entities çš„æ˜ å°„
    chunk_to_entities_map = create_chunk_to_entities_map(disambiguated_entities)
    
    extracted_triples = []
    
    print(f"ğŸ”— å¼€å§‹å¤„ç† {len(chunk_to_entities_map)} ä¸ªæ–‡æœ¬å—çš„å…³ç³»æŠ½å–")
    
    for chunk_id, entities in chunk_to_entities_map.items():
        chunk_text = all_chunks.get(chunk_id, "")
        if not chunk_text:
            print(f"âš ï¸ Chunk {chunk_id} åœ¨chunkæ•°æ®ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡")
            continue
        
        if len(entities) < 2:
            continue  # å®ä½“æ•°é‡å°‘äº2ä¸ªï¼Œæ— æ³•å½¢æˆå…³ç³»
        
        # ä¸ºå½“å‰å—æŠ½å–å…³ç³»
        triples_from_chunk = extract_relations_for_chunk(chunk_id, chunk_text, entities)

        validated_triples = []
        # å°†å®ä½“åˆ—è¡¨è½¬ä¸ºé›†åˆï¼Œä¾¿äºå¿«é€ŸæŸ¥æ‰¾
        valid_entity_set = set(entities)        

        for triple in triples_from_chunk:
            if len(triple) == 3:
                head, relation, tail = triple
                # æ£€æŸ¥å¤´å®ä½“å’Œå°¾å®ä½“æ˜¯å¦éƒ½åœ¨åˆæ³•çš„å®ä½“åˆ—è¡¨ä¸­
                if head in valid_entity_set and tail in valid_entity_set:
                    validated_triples.append(triple)
                else:
                    print(f"ğŸš« [å·²è¿‡æ»¤] å‘ç°å¹»è§‰å®ä½“ï¼Œå·²ä¸¢å¼ƒ: {triple}")
        
        # ä¸ºæ¯ä¸ªä¸‰å…ƒç»„æ·»åŠ æ¥æºä¿¡æ¯
        for triple in validated_triples:
            if len(triple) == 3:
                extracted_triples.append({
                    "head": triple[0],
                    "relation": triple[1],
                    "tail": triple[2],
                    "source_chunk_id": chunk_id
                })
    
    # å»é‡å¤„ç†
    unique_triples_str = {json.dumps(d, sort_keys=True) for d in extracted_triples}
    final_triples = [json.loads(s) for s in unique_triples_str]
    
    # ä¿å­˜æ‰€æœ‰å…³ç³»æŠ½å–ç»“æœåˆ°ç»Ÿä¸€æ–‡ä»¶
    output_path = os.path.join(config.RE_OUTPUT_DIR, "all_relations.json")
    save_json(final_triples, output_path)
    
    print(f"âœ… å…³ç³»æŠ½å–å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ğŸ·ï¸ å¤„ç†å®ä½“æ•°: {len(disambiguated_entities)}")
    print(f"   ğŸ“„ å¤„ç†chunkæ•°: {len(chunk_to_entities_map)}")
    print(f"   ğŸ”— æŠ½å–å…³ç³»æ•°: {len(final_triples)}")
    print(f"   ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_path}")
    
    return output_path

if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config.RE_OUTPUT_DIR, exist_ok=True)
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶çš„å…³ç³»æŠ½å–
    result = run_relation_extraction_on_all()
    
    if result:
        print("\nğŸ‰ æ‰€æœ‰æ–‡ä»¶çš„å…³ç³»æŠ½å–å¤„ç†å®Œæˆï¼")
    else:
        print("\nâŒ å…³ç³»æŠ½å–å¤„ç†å¤±è´¥ï¼")