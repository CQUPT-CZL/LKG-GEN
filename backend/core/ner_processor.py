import os
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
import config
from .utils import call_llm, save_json, load_prompt, load_json

def validate_entity_with_llm(entity, context_text, validation_prompt_template):
    """ä½¿ç”¨å¤§æ¨¡å‹éªŒè¯å®ä½“æ˜¯å¦åˆç†"""
    try:
        # åŸºç¡€å­—æ®µæ£€æŸ¥
        if not isinstance(entity, dict):
            return None
        
        required_fields = ['entity_text', 'entity_type', 'entity_description']
        for field in required_fields:
            if field not in entity or not entity[field]:
                return None
        
        # æ„å»ºéªŒè¯prompt
        prompt = validation_prompt_template.replace("{{ENTITY_NAME}}", str(entity['entity_text']))
        prompt = prompt.replace("{{ENTITY_TYPE}}", str(entity['entity_type']))
        prompt = prompt.replace("{{ENTITY_DESCRIPTION}}", str(entity['entity_description']))
        prompt = prompt.replace("{{CONTEXT_TEXT}}", str(context_text))
        
        # è°ƒç”¨å¤§æ¨¡å‹è¿›è¡ŒéªŒè¯
        validation_result = call_llm(prompt, model_name="qwen-max")
        # print(f"å¤§æ¨¡å‹éªŒè¯ç»“æœ: {validation_result}")
        
        if validation_result and isinstance(validation_result, dict):
            if validation_result.get('is_valid', False):
                # å¦‚æœæœ‰ä¿®æ­£å»ºè®®ï¼Œä½¿ç”¨ä¿®æ­£åçš„å®ä½“
                corrected = validation_result.get('corrected_entity')
                if corrected and isinstance(corrected, dict):
                    # æ›´æ–°å®ä½“ä¿¡æ¯
                    if corrected.get('name'):
                        entity['entity_text'] = corrected['name']
                    if corrected.get('type'):
                        entity['entity_type'] = corrected['type']
                    if corrected.get('description'):
                        entity['entity_description'] = corrected['description']
                return entity
            else:
                # å®ä½“ä¸åˆç†ï¼Œè¿”å›Noneè¡¨ç¤ºåˆ é™¤
                print(f"ğŸ—‘ï¸ åˆ é™¤ä¸åˆç†å®ä½“: {entity['entity_text']} - {validation_result.get('reason', 'æœªçŸ¥åŸå› ')}")
                return None
        else:
            # éªŒè¯å¤±è´¥ï¼Œä¿å®ˆèµ·è§ä¿ç•™å®ä½“
            print(f"âš ï¸ å®ä½“éªŒè¯å¤±è´¥ï¼Œä¿ç•™å®ä½“: {entity['entity_text']}")
            return entity
            
    except Exception as e:
        print(f"âŒ éªŒè¯å®ä½“ {entity.get('entity_text', 'unknown')} æ—¶å‡ºé”™: {e}")
        # å‡ºé”™æ—¶ä¿ç•™å®ä½“
        return entity

import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from functools import partial

# çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤å…±äº«èµ„æº
result_lock = threading.Lock()

def process_single_chunk(chunk_id, chunk_text, prompt_template, validation_prompt_template, file_prefix):
    """å¤„ç†å•ä¸ªchunkçš„NERä»»åŠ¡ï¼ŒåŒ…å«å®ä½“éªŒè¯"""
    try:
        # 1. æ‰§è¡ŒNERæå–
        prompt = prompt_template.replace("{{ENTITY_TYPES}}", str(config.ENTITY_TYPES))
        prompt = prompt.replace("{{TEXT_CONTENT}}", chunk_text)
        chunk_ner = call_llm(prompt, model_name="qwen-max")
        
        if chunk_ner and isinstance(chunk_ner, list):
            validated_entities = []
            original_count = len(chunk_ner)
            
            # 2. å¯¹æ¯ä¸ªå®ä½“è¿›è¡ŒéªŒè¯
            for entity in chunk_ner:
                # ä¸ºchunk_idæ·»åŠ æ–‡ä»¶åå‰ç¼€ï¼Œæ ¼å¼ï¼šæ–‡ä»¶å_chunk_id
                entity['chunk_id'] = [f"{file_prefix}_{chunk_id}"]
                entity['category_path'] = [f"{file_prefix}_{chunk_id}"]
                
                # éªŒè¯å®ä½“
                validated_entity = validate_entity_with_llm(entity, chunk_text, validation_prompt_template)
                if validated_entity is not None:
                    validated_entities.append(validated_entity)
            
            # 3. è¾“å‡ºéªŒè¯ç»Ÿè®¡
            validated_count = len(validated_entities)
            print("---->")
            print(f"Chunk {chunk_id}: åŸå§‹å®ä½“ {original_count} ä¸ªï¼ŒéªŒè¯åä¿ç•™ {validated_count} ä¸ª")
            print("----<")
            removed_count = original_count - validated_count
            if removed_count > 0:
                print(f"ğŸ“Š Chunk {chunk_id}: åŸå§‹å®ä½“ {original_count} ä¸ªï¼ŒéªŒè¯åä¿ç•™ {validated_count} ä¸ªï¼Œåˆ é™¤ {removed_count} ä¸ª")
            
            return validated_entities
        return []
    except Exception as e:
        print(f"âŒ å¤„ç†chunk {chunk_id} æ—¶å‡ºé”™: {e}")
        return []

def run_ner_on_file(chunk_filepath, max_workers=4):
    """å¯¹å•ä¸ªåˆ†å—æ–‡ä»¶è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼ŒåŒ…å«å®ä½“éªŒè¯ï¼‰"""
    print(f"ğŸ”„ æ­£åœ¨å¤„ç†åˆ†å—æ–‡ä»¶: {chunk_filepath}")
    
    # 1. åŠ è½½åˆ†å—æ•°æ®
    chunks = load_json(chunk_filepath)
    if not chunks:
        print("âš ï¸ åˆ†å—æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
        return None

    ner_results = []
    prompt_template = load_prompt(config.NER_PROMPT_PATH)
    
    # åŠ è½½å®ä½“éªŒè¯promptæ¨¡æ¿
    validation_prompt_template = load_prompt(config.ENTITY_VALIDATION_PROMPT_PATH)

    # è·å–æ–‡ä»¶åå‰ç¼€ï¼ˆå»æ‰.jsonæ‰©å±•åï¼‰
    file_prefix = os.path.basename(chunk_filepath).replace('.json', '')
    
    # 2. ä½¿ç”¨å¤šçº¿ç¨‹å¯¹æ¯ä¸ªchunkè¿›è¡ŒNERå’ŒéªŒè¯
    print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(chunks)} ä¸ªchunksï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹ï¼ˆåŒ…å«å®ä½“éªŒè¯ï¼‰")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_chunk = {
            executor.submit(process_single_chunk, chunk_id, chunk_text, prompt_template, validation_prompt_template, file_prefix): chunk_id
            for chunk_id, chunk_text in chunks.items()
        }
        
        # æ”¶é›†ç»“æœ
        with tqdm.tqdm(total=len(chunks), desc=f"å¤„ç† {os.path.basename(chunk_filepath)}") as pbar:
            for future in as_completed(future_to_chunk):
                chunk_id = future_to_chunk[future]
                try:
                    chunk_result = future.result()
                    with result_lock:
                        ner_results.extend(chunk_result)
                except Exception as e:
                    print(f"âŒ å¤„ç†chunk {chunk_id} æ—¶å‡ºç°å¼‚å¸¸: {e}")
                finally:
                    pbar.update(1)

    # 3. ä¿å­˜NERç»“æœå¹¶è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    if ner_results:
        filename = os.path.basename(chunk_filepath)
        output_filepath = os.path.join(config.NER_OUTPUT_DIR, filename)
        save_json(ner_results, output_filepath)
        print(f"âœ… NERç»“æœå·²ä¿å­˜åˆ°: {output_filepath}")
        print(f"ğŸ“ˆ æ–‡ä»¶ {filename} éªŒè¯åå…±ä¿ç•™ {len(ner_results)} ä¸ªæœ‰æ•ˆå®ä½“")
        return output_filepath
    else:
        print("âš ï¸ æœªèƒ½ç”ŸæˆNERç»“æœã€‚")
        return None

def process_file_wrapper(chunk_path, max_workers_per_file=4):
    """æ–‡ä»¶å¤„ç†åŒ…è£…å™¨"""
    try:
        result = run_ner_on_file(chunk_path, max_workers_per_file)
        if result:
            return f"âœ… æˆåŠŸå¤„ç†: {os.path.basename(chunk_path)}"
        else:
            return f"âš ï¸ å¤„ç†å¤±è´¥: {os.path.basename(chunk_path)}"
    except Exception as e:
        return f"âŒ å¤„ç†å¼‚å¸¸: {os.path.basename(chunk_path)} - {e}"

def process_all_files(max_file_workers=2, max_chunk_workers=4):
    """å¤„ç†æ‰€æœ‰åˆ†å—æ–‡ä»¶çš„NERä»»åŠ¡"""
    print("ğŸš€ å¼€å§‹å¤šçº¿ç¨‹NERå¤„ç†...")
    print(f"ğŸ“‹ é…ç½®: æ–‡ä»¶å¹¶å‘æ•°={max_file_workers}, æ¯æ–‡ä»¶chunkå¹¶å‘æ•°={max_chunk_workers}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config.NER_OUTPUT_DIR, exist_ok=True)
    
    # è·å–æ‰€æœ‰åˆ†å—æ–‡ä»¶
    chunk_files = []
    for filename in os.listdir(config.CHUNK_OUTPUT_DIR):
        if filename.endswith(".json"):
            chunk_path = os.path.join(config.CHUNK_OUTPUT_DIR, filename)
            chunk_files.append(chunk_path)
    
    if not chunk_files:
        print("âš ï¸ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„åˆ†å—æ–‡ä»¶")
        return []
    
    print(f"ğŸ“ æ‰¾åˆ° {len(chunk_files)} ä¸ªåˆ†å—æ–‡ä»¶å¾…å¤„ç†")
    
    # ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶
    with ThreadPoolExecutor(max_workers=max_file_workers) as executor:
        # åˆ›å»ºéƒ¨åˆ†å‡½æ•°ï¼Œå›ºå®šmax_workers_per_fileå‚æ•°
        process_func = partial(process_file_wrapper, max_workers_per_file=max_chunk_workers)
        
        # æäº¤æ‰€æœ‰æ–‡ä»¶å¤„ç†ä»»åŠ¡
        future_to_file = {
            executor.submit(process_func, chunk_path): chunk_path
            for chunk_path in chunk_files
        }
        
        # æ”¶é›†ç»“æœ
        results = []
        with tqdm.tqdm(total=len(chunk_files), desc="å¤„ç†æ–‡ä»¶") as pbar:
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    error_msg = f"âŒ å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºç°å¼‚å¸¸: {e}"
                    print(error_msg)
                    results.append(error_msg)
                finally:
                    pbar.update(1)
    
    # ç»Ÿè®¡ç»“æœ
    success_count = sum(1 for r in results if "âœ…" in r)
    error_count = len(results) - success_count
    
    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰åˆ†å—æ–‡ä»¶çš„å‘½åå®ä½“è¯†åˆ«å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {error_count} ä¸ª")
    if error_count > 0:
        print("âŒ å¤±è´¥çš„æ–‡ä»¶:")
        for result in results:
            if "âŒ" in result:
                print(f"   {result}")
    print("="*50)
    
    return results

if __name__ == "__main__":
    MAX_FILE_WORKERS = 2  # åŒæ—¶å¤„ç†çš„æ–‡ä»¶æ•°é‡
    MAX_CHUNK_WORKERS = 4  # æ¯ä¸ªæ–‡ä»¶å†…åŒæ—¶å¤„ç†çš„chunkæ•°é‡
    process_all_files(MAX_FILE_WORKERS, MAX_CHUNK_WORKERS)