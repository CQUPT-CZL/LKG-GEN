import os
import json
from collections import defaultdict
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(__file__)))
import config
from .utils import call_llm, load_json, save_json, load_prompt
from difflib import SequenceMatcher
import itertools
from concurrent.futures import ThreadPoolExecutor, as_completed
import tqdm

def calculate_similarity(text1, text2):
    """è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦"""
    return SequenceMatcher(None, text1.lower(), text2.lower()).ratio()

def group_entities_by_type(entities):
    """æŒ‰å®ä½“ç±»å‹åˆ†ç»„"""
    type_groups = defaultdict(list)
    for entity in entities:
        type_groups[entity["entity_type"]].append(entity)
    return type_groups

def find_similar_entities(entities, similarity_threshold=0.6):
    """åŸºäºæ–‡æœ¬ç›¸ä¼¼åº¦é¢„ç­›é€‰å¯èƒ½éœ€è¦åˆå¹¶çš„å®ä½“ç»„"""
    similar_groups = []
    processed = set()
    
    for i, entity1 in enumerate(entities):
        if i in processed:
            continue
            
        current_group = [entity1]
        processed.add(i)
        
        for j, entity2 in enumerate(entities[i+1:], i+1):
            if j in processed:
                continue
                
            similarity = calculate_similarity(entity1["entity_text"], entity2["entity_text"])
            if similarity >= similarity_threshold:
                current_group.append(entity2)
                processed.add(j)
        
        similar_groups.append(current_group)
    
    return similar_groups

def smart_entity_filtering(entities, min_frequency=2):
    """æ™ºèƒ½å®ä½“ç­›é€‰ï¼šä¼˜å…ˆå¤„ç†é«˜é¢‘å®ä½“ï¼Œè¿‡æ»¤å™ªå£°"""
    # ç»Ÿè®¡å®ä½“é¢‘ç‡ï¼ˆåŸºäºchunk_idæ•°é‡ï¼‰
    entity_freq = {}
    for entity in entities:
        text = entity["entity_text"]
        freq = len(entity.get("chunk_id", []))
        if text in entity_freq:
            entity_freq[text] += freq
        else:
            entity_freq[text] = freq
    
    # æŒ‰é¢‘ç‡åˆ†ç»„
    high_freq_entities = []
    low_freq_entities = []
    
    for entity in entities:
        text = entity["entity_text"]
        if entity_freq[text] >= min_frequency:
            high_freq_entities.append(entity)
        else:
            low_freq_entities.append(entity)
    
    print(f"   ğŸ“Š å®ä½“ç­›é€‰: é«˜é¢‘å®ä½“ {len(high_freq_entities)} ä¸ªï¼Œä½é¢‘å®ä½“ {len(low_freq_entities)} ä¸ª")
    
    return high_freq_entities, low_freq_entities

def disambiguate_entities_with_llm(entity_list: list, max_entities_per_batch=20):
    """
    ä½¿ç”¨LLMå¯¹å®ä½“åˆ—è¡¨è¿›è¡Œåˆ†æ‰¹èšç±»æ¶ˆæ­§ã€‚
    é‡‡ç”¨åˆ†æ‰¹å¤„ç†ç­–ç•¥é¿å…ä¸Šä¸‹æ–‡è¿‡é•¿å¯¼è‡´çš„æ•ˆæœä¸‹é™ã€‚
    """
    if len(entity_list) <= max_entities_per_batch:
        # å¦‚æœå®ä½“æ•°é‡ä¸å¤šï¼Œç›´æ¥å¤„ç†
        return _process_single_batch(entity_list)
    
    print(f"ğŸ”„ å®ä½“æ•°é‡è¾ƒå¤š({len(entity_list)})ï¼Œé‡‡ç”¨åˆ†æ‰¹å¤„ç†ç­–ç•¥...")
    
    # 1. æŒ‰ç›¸ä¼¼åº¦é¢„åˆ†ç»„
    similar_groups = find_similar_entities(entity_list, similarity_threshold=0.6)
    print(f"ğŸ“Š åŸºäºç›¸ä¼¼åº¦é¢„åˆ†ç»„ï¼Œå¾—åˆ° {len(similar_groups)} ä¸ªå€™é€‰ç»„")
    
    all_clusters = []
    
    # 2. å¯¹æ¯ä¸ªç›¸ä¼¼ç»„è¿›è¡Œå¤„ç†
    for i, group in enumerate(tqdm.tqdm(similar_groups, desc="å¤„ç†ç›¸ä¼¼ç»„")):
        if len(group) == 1:
            # å•ä¸ªå®ä½“ï¼Œæ— éœ€èšç±»
            continue
        elif len(group) <= max_entities_per_batch:
            # ç»„å†…å®ä½“æ•°é‡é€‚ä¸­ï¼Œç›´æ¥å¤„ç†
            batch_clusters = _process_single_batch(group)
            if batch_clusters:
                all_clusters.extend(batch_clusters)
        else:
            # ç»„å†…å®ä½“è¿‡å¤šï¼Œè¿›ä¸€æ­¥åˆ†æ‰¹
            for j in range(0, len(group), max_entities_per_batch):
                batch = group[j:j + max_entities_per_batch]
                batch_clusters = _process_single_batch(batch)
                if batch_clusters:
                    all_clusters.extend(batch_clusters)
    
    return all_clusters

def _process_single_batch(entity_list: list):
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„å®ä½“èšç±»
    """
    if len(entity_list) <= 1:
        return None
        
    try:
        prompt_template = load_prompt(config.DISAMBIGUATION_PROMPT_PATH)
        
        # ä¸ºäº†è®©Promptæ›´ç®€æ´ï¼Œå¯ä»¥åªå‘é€å¿…è¦çš„å­—æ®µç»™LLM
        simplified_entities = [
            {"entity_text": e["entity_text"], "entity_description": e["entity_description"]} 
            for e in entity_list
        ]
        prompt = prompt_template.replace("{{ENTITY_LIST_JSON}}", json.dumps(simplified_entities, ensure_ascii=False, indent=2))
        
        llm_response = call_llm(prompt, model_name="qwen-plus-latest")
        
        # å…³é”®ï¼šæˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºæ˜¯åŒ…å« "clusters" é”®çš„å­—å…¸
        if llm_response and "clusters" in llm_response and isinstance(llm_response["clusters"], list):
            return llm_response["clusters"]
        else:
            print(f"âš ï¸ LLMè¿”å›æ ¼å¼ä¸æ­£ç¡®ï¼Œæ‰¹æ¬¡å¤§å°: {len(entity_list)}")
            return None
    except Exception as e:
        print(f"âŒ å¤„ç†æ‰¹æ¬¡æ—¶å‡ºé”™: {e}")
        return None

def process_llm_clusters(original_entities: list, clusters: list):
    """
    æ ¹æ®LLMè¿”å›çš„èšç±»ç»“æœï¼Œç²¾ç¡®åœ°åˆå¹¶åŸå§‹å®ä½“åˆ—è¡¨ï¼Œç‰¹åˆ«æ˜¯chunk_idã€‚
    """
    if not clusters:
        print("æ²¡æœ‰èšç±»ä¿¡æ¯ï¼Œè¿”å›åŸå§‹å®ä½“åˆ—è¡¨ã€‚")
        return original_entities

    print("ğŸ”§ æ­£åœ¨æ ¹æ®èšç±»é…æ–¹ï¼Œåœ¨Pythonä¸­ç²¾ç¡®åˆå¹¶å®ä½“...")
    
    final_entities = []
    # åˆ›å»ºä¸€ä¸ªä» entity_text åˆ°å…¶å®Œæ•´å¯¹è±¡çš„æ˜ å°„ï¼Œæ”¯æŒä¸€ä¸ªtextå¯¹åº”å¤šä¸ªå¯¹è±¡ï¼ˆå› ä¸ºè¾“å…¥ä¸­å¯èƒ½æœ‰é‡å¤ï¼‰
    entity_map = defaultdict(list)
    for e in original_entities:
        entity_map[e["entity_text"]].append(e)

    processed_texts = set()

    for cluster in clusters:
        aliases = cluster.get("aliases", [])
        canonical_name = cluster.get("canonical_name")

        if not aliases or not canonical_name:
            continue
        
        # ç¡®ä¿è§„èŒƒåç§°æœ¬èº«ä¹Ÿåœ¨åˆ«ååˆ—è¡¨ä¸­ï¼Œä¾¿äºå¤„ç†
        if canonical_name not in aliases:
            aliases.append(canonical_name)

        merged_chunk_ids = set()
        
        # æ‰¾åˆ°è§„èŒƒåç§°å¯¹åº”çš„é‚£ä¸ªå®ä½“ï¼Œä»¥å®ƒçš„ç±»å‹å’Œæè¿°ä¸ºå‡†
        # ä¼˜å…ˆä½¿ç”¨ä¸canonical_nameå®Œå…¨åŒ¹é…çš„å®ä½“ä½œä¸ºæ¨¡æ¿
        canonical_entity_template = entity_map.get(canonical_name, [None])[0]
        if not canonical_entity_template:
            # å¦‚æœLLMç”Ÿæˆçš„è§„èŒƒåä¸åœ¨åŸæ–‡ä¸­ï¼Œå°±ç”¨åˆ«ååˆ—è¡¨é‡Œçš„ç¬¬ä¸€ä¸ª
            canonical_entity_template = entity_map.get(aliases[0], [None])[0]
            if not canonical_entity_template: continue # æç«¯æƒ…å†µï¼Œè·³è¿‡

        for alias in aliases:
            # ä¸€ä¸ªåˆ«åå¯èƒ½å¯¹åº”å¤šä¸ªå®ä½“å¯¹è±¡ï¼ˆä¾‹å¦‚ "ç‚¼é“ç³»ç»Ÿ" åœ¨åŸæ–‡å‡ºç°å¤šæ¬¡ï¼‰
            if alias in entity_map:
                for entity_obj in entity_map[alias]:
                    merged_chunk_ids.update(entity_obj["chunk_id"])
                processed_texts.add(alias)
        
        # åˆ›å»ºåˆå¹¶åçš„æ–°å®ä½“
        new_entity = {
            "entity_text": canonical_name,
            "entity_type": canonical_entity_template["entity_type"],
            "entity_description": canonical_entity_template["entity_description"],
            "chunk_id": sorted(list(merged_chunk_ids)), # åˆå¹¶å¹¶æ’åºchunk_id
            "aliases": aliases 
        }
        final_entities.append(new_entity)

    # æ·»åŠ é‚£äº›æœªè¢«èšç±»çš„ç‹¬ç«‹å®ä½“
    for entity_text, entity_objects in entity_map.items():
        if entity_text not in processed_texts:
            # å³ä½¿æ˜¯ç‹¬ç«‹å®ä½“ï¼Œä¹Ÿå¯èƒ½åœ¨åŸæ–‡å‡ºç°å¤šæ¬¡ï¼Œéœ€è¦åˆå¹¶å…¶chunk_id
            merged_chunk_ids = set()
            for entity_obj in entity_objects:
                merged_chunk_ids.update(entity_obj["chunk_id"])
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯¹è±¡ä½œä¸ºæ¨¡æ¿
            template_obj = entity_objects[0].copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå¯¹è±¡
            template_obj["chunk_id"] = sorted(list(merged_chunk_ids))
            final_entities.append(template_obj)

    print(f"âœ… å®ä½“åˆå¹¶å®Œæˆã€‚åŸå§‹å®ä½“æ•°: {len(original_entities)}, åˆå¹¶åå®ä½“æ•°: {len(final_entities)}")
    return final_entities

def run_disambiguate_on_all_files():
    """å¯¹æ•´ä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰NERæ–‡ä»¶è¿›è¡Œå®ä½“æ¶ˆæ­§ï¼Œé‡‡ç”¨åˆ†ç±»å‹åˆ†æ‰¹å¤„ç†ç­–ç•¥"""
    print("ğŸš€ å¼€å§‹æ™ºèƒ½å®ä½“æ¶ˆæ­§å¤„ç†...")
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„å®ä½“
    all_entities = []
    processed_files = []
    
    # éå†æ‰€æœ‰NERè¾“å‡ºæ–‡ä»¶
    for filename in os.listdir(config.NER_OUTPUT_DIR):
        if filename.endswith(".json"):
            input_path = os.path.join(config.NER_OUTPUT_DIR, filename)
            print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡ä»¶: {filename}")
            
            entities = load_json(input_path)
            if entities:
                all_entities.extend(entities)
                processed_files.append(filename)
                print(f"âœ… ä» {filename} åŠ è½½äº† {len(entities)} ä¸ªå®ä½“")
            else:
                print(f"âš ï¸ æ–‡ä»¶ {filename} å®ä½“æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
    
    if not all_entities:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“æ•°æ®")
        return None
    
    print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_entities)} ä¸ªå®ä½“ï¼Œæ¥è‡ª {len(processed_files)} ä¸ªæ–‡ä»¶")
    
    # 1. æŒ‰å®ä½“ç±»å‹åˆ†ç»„å¤„ç†
    print("ğŸ” æŒ‰å®ä½“ç±»å‹åˆ†ç»„å¤„ç†...")
    type_groups = group_entities_by_type(all_entities)
    
    print(f"ğŸ“‹ å‘ç° {len(type_groups)} ç§å®ä½“ç±»å‹:")
    for entity_type, entities in type_groups.items():
        print(f"   ğŸ·ï¸ {entity_type}: {len(entities)} ä¸ªå®ä½“")
    
    # 2. åˆ†ç±»å‹è¿›è¡Œæ¶ˆæ­§å¤„ç†
    all_clusters = []
    type_stats = {}
    
    for entity_type, entities in type_groups.items():
         print(f"\nğŸ”„ æ­£åœ¨å¤„ç†ç±»å‹: {entity_type} ({len(entities)} ä¸ªå®ä½“)")
         
         if len(entities) <= 1:
             print(f"   â­ï¸ å®ä½“æ•°é‡è¿‡å°‘ï¼Œè·³è¿‡æ¶ˆæ­§")
             type_stats[entity_type] = {"original": len(entities), "clusters": 0, "processed": len(entities)}
             continue
         
         # æ™ºèƒ½ç­›é€‰ï¼šä¼˜å…ˆå¤„ç†é«˜é¢‘å®ä½“
         high_freq_entities, low_freq_entities = smart_entity_filtering(entities, min_frequency=2)
         
         type_clusters = []
         
         # ä¼˜å…ˆå¤„ç†é«˜é¢‘å®ä½“ï¼ˆæ›´å¯èƒ½éœ€è¦æ¶ˆæ­§ï¼‰
         if high_freq_entities:
             print(f"   ğŸ¯ ä¼˜å…ˆå¤„ç†é«˜é¢‘å®ä½“ ({len(high_freq_entities)} ä¸ª)")
             high_freq_clusters = disambiguate_entities_with_llm(high_freq_entities, max_entities_per_batch=12)
             if high_freq_clusters:
                 type_clusters.extend(high_freq_clusters)
                 print(f"   âœ… é«˜é¢‘å®ä½“ç”Ÿæˆäº† {len(high_freq_clusters)} ä¸ªèšç±»")
         
         # å¤„ç†ä½é¢‘å®ä½“ï¼ˆæ‰¹é‡è¾ƒå¤§ï¼Œå‡å°‘APIè°ƒç”¨ï¼‰
         if low_freq_entities and len(low_freq_entities) >= 3:  # åªæœ‰è¶³å¤Ÿæ•°é‡æ‰å¤„ç†
             print(f"   ğŸ” å¤„ç†ä½é¢‘å®ä½“ ({len(low_freq_entities)} ä¸ª)")
             low_freq_clusters = disambiguate_entities_with_llm(low_freq_entities, max_entities_per_batch=20)
             if low_freq_clusters:
                 type_clusters.extend(low_freq_clusters)
                 print(f"   âœ… ä½é¢‘å®ä½“ç”Ÿæˆäº† {len(low_freq_clusters)} ä¸ªèšç±»")
         
         if type_clusters:
             all_clusters.extend(type_clusters)
             type_stats[entity_type] = {
                 "original": len(entities), 
                 "clusters": len(type_clusters), 
                 "high_freq": len(high_freq_entities),
                 "low_freq": len(low_freq_entities)
             }
             print(f"   ğŸ‰ ç±»å‹ {entity_type} æ€»å…±ç”Ÿæˆäº† {len(type_clusters)} ä¸ªèšç±»")
         else:
             print(f"   âš ï¸ è¯¥ç±»å‹æœªç”Ÿæˆæœ‰æ•ˆèšç±»")
             type_stats[entity_type] = {
                 "original": len(entities), 
                 "clusters": 0, 
                 "high_freq": len(high_freq_entities),
                 "low_freq": len(low_freq_entities)
             }
    
    # 3. åœ¨Pythonä¸­æ ¹æ®æ‰€æœ‰èšç±»é…æ–¹è¿›è¡Œå¤„ç†
    print(f"\nğŸ”§ æ­£åœ¨æ ¹æ® {len(all_clusters)} ä¸ªèšç±»ç»“æœåˆå¹¶å®ä½“...")
    final_disambiguated_entities = process_llm_clusters(all_entities, all_clusters)
    
    # 4. ä¿å­˜åˆå¹¶åçš„ç»“æœåˆ°å•ä¸ªæ–‡ä»¶
    output_filepath = os.path.join(config.NER_PRO_OUTPUT_DIR, "all_entities_disambiguated.json")
    save_json(final_disambiguated_entities, output_filepath)
    
    # 5. è¾“å‡ºè¯¦ç»†ç»Ÿè®¡ä¿¡æ¯
    print(f"\n" + "="*60)
    print(f"ğŸ‰ å®ä½“æ¶ˆæ­§å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ˆ æ€»ä½“ç»Ÿè®¡:")
    print(f"   ğŸ“„ å¤„ç†æ–‡ä»¶æ•°: {len(processed_files)}")
    print(f"   ğŸ·ï¸ åŸå§‹å®ä½“æ•°: {len(all_entities)}")
    print(f"   ğŸ”— æ¶ˆæ­§åå®ä½“æ•°: {len(final_disambiguated_entities)}")
    print(f"   ğŸ“‰ å‹ç¼©ç‡: {(1 - len(final_disambiguated_entities)/len(all_entities))*100:.1f}%")
    print(f"   ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_filepath}")
    
    print(f"\nğŸ“Š åˆ†ç±»å‹è¯¦ç»†ç»Ÿè®¡:")
    for entity_type, stats in type_stats.items():
        if 'high_freq' in stats:
            print(f"   ğŸ·ï¸ {entity_type}: {stats['original']} ä¸ªå®ä½“ â†’ {stats['clusters']} ä¸ªèšç±»")
            print(f"      ğŸ“ˆ é«˜é¢‘: {stats['high_freq']} ä¸ª, ä½é¢‘: {stats['low_freq']} ä¸ª")
        else:
            print(f"   ğŸ·ï¸ {entity_type}: {stats['original']} ä¸ªå®ä½“ â†’ {stats['clusters']} ä¸ªèšç±»")
    print("="*60)
    
    return output_filepath

def simple_entity_disambiguation(entities):
    """ç®€åŒ–ç‰ˆå®ä½“æ¶ˆæ­§ï¼Œç”¨äºå¿«é€Ÿå¤„ç†"""
    return run_disambiguate_on_all_files()

def ensure_output_files_exist():
    """ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨"""
    directories = [
        config.DATA_DIR,
        config.RAW_PAPERS_DIR,
        config.PROCESSED_TEXT_DIR,
        config.CHUNK_OUTPUT_DIR,
        config.NER_OUTPUT_DIR,
        config.NER_PRO_OUTPUT_DIR,
        config.RE_OUTPUT_DIR,
        config.GRAPH_TRIPLES_DIR
    ]
    
    for directory in directories:
        os.makedirs(directory, exist_ok=True)
    
    print("âœ… æ‰€æœ‰å¿…è¦çš„è¾“å‡ºç›®å½•å·²åˆ›å»º")

if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    ensure_output_files_exist()
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶ç»“æœ
    result = run_disambiguate_on_all_files()
    
    if result:
        print("\nğŸ‰ æ‰€æœ‰NERæ–‡ä»¶çš„å®ä½“æ¶ˆæ­§å¤„ç†å®Œæˆï¼")
    else:
        print("\nâŒ å®ä½“æ¶ˆæ­§å¤„ç†å¤±è´¥ï¼")