import os
import sys
import json
import asyncio
import shutil
from pathlib import Path
from typing import Dict, Any, List, Callable, Optional
from datetime import datetime

# å¯¼å…¥é‡æ„åçš„æ ¸å¿ƒæ¨¡å—
try:
    from core import (
        config, save_json, load_json, load_text, load_prompt, call_llm,
        run_chunk_on_file, run_ner_on_file, run_relation_extraction_on_all,
        simple_entity_disambiguation, ensure_output_files_exist,
        run_disambiguate_on_all_files
    )
except ImportError as e:
    print(f"å¯¼å…¥çŸ¥è¯†å›¾è°±æ„å»ºæ ¸å¿ƒæ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿backend/coreç›®å½•ä¸‹çš„æ¨¡å—å¯ç”¨")

from data_manager import DataManager

class KnowledgeGraphBuilder:
    """çŸ¥è¯†å›¾è°±æ„å»ºå™¨ - æ•´åˆç°æœ‰çš„Pythonè„šæœ¬"""
    
    def __init__(self, data_manager=None):
        self.data_manager = data_manager or DataManager()
        
        # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
        self.temp_dir = Path("temp")
        self.temp_dir.mkdir(exist_ok=True)
        
        # å½“å‰å·¥ä½œç›®å½•ï¼ˆç”¨äºç‹¬ç«‹æ„å»ºæ¨¡å¼ï¼‰
        self.current_work_dir = None
        
        # ç¡®ä¿src/dataç›®å½•ç»“æ„å­˜åœ¨
        self._ensure_data_directories()
        
        # ç¡®ä¿è¾“å‡ºæ–‡ä»¶å­˜åœ¨
        ensure_output_files_exist()
    
    def _ensure_data_directories(self):
        """ç¡®ä¿æ•°æ®ç›®å½•ç»“æ„å­˜åœ¨"""
        try:
            # ä½¿ç”¨configä¸­å®šä¹‰çš„ç›®å½•
            directories = [
                config.PROCESSED_TEXT_DIR,
                config.CHUNK_OUTPUT_DIR,
                config.NER_OUTPUT_DIR,
                config.NER_PRO_OUTPUT_DIR,
                config.RE_OUTPUT_DIR
            ]
            
            for directory in directories:
                os.makedirs(directory, exist_ok=True)
                
        except Exception as e:
            print(f"åˆ›å»ºæ•°æ®ç›®å½•å¤±è´¥: {e}")
    

    
    def _get_work_directories(self, graph_id: str) -> Dict[str, str]:
        """æ ¹æ®å›¾è°±IDè·å–å·¥ä½œç›®å½•"""
        if not graph_id:
            raise ValueError("å›¾è°±IDä¸èƒ½ä¸ºç©º")
        
        # æ‰€æœ‰å›¾è°±éƒ½ä½¿ç”¨å›¾è°±ç‰¹å®šçš„å­ç›®å½•
        return config.get_graph_data_dirs(graph_id)
    
    def _update_config_paths(self, work_dirs: Dict[str, str]):
        """ä¸´æ—¶æ›´æ–°configä¸­çš„è·¯å¾„"""
        self.original_paths = {
            "PROCESSED_TEXT_DIR": config.PROCESSED_TEXT_DIR,
            "CHUNK_OUTPUT_DIR": config.CHUNK_OUTPUT_DIR,
            "NER_OUTPUT_DIR": config.NER_OUTPUT_DIR,
            "NER_PRO_OUTPUT_DIR": config.NER_PRO_OUTPUT_DIR,
            "RE_OUTPUT_DIR": config.RE_OUTPUT_DIR
        }
        
        # æ›´æ–°configè·¯å¾„
        config.PROCESSED_TEXT_DIR = work_dirs["PROCESSED_TEXT_DIR"]
        config.CHUNK_OUTPUT_DIR = work_dirs["CHUNK_OUTPUT_DIR"]
        config.NER_OUTPUT_DIR = work_dirs["NER_OUTPUT_DIR"]
        config.NER_PRO_OUTPUT_DIR = work_dirs["NER_PRO_OUTPUT_DIR"]
        config.RE_OUTPUT_DIR = work_dirs["RE_OUTPUT_DIR"]
    
    def _restore_config_paths(self):
        """æ¢å¤configä¸­çš„åŸå§‹è·¯å¾„"""
        if hasattr(self, 'original_paths'):
            config.PROCESSED_TEXT_DIR = self.original_paths["PROCESSED_TEXT_DIR"]
            config.CHUNK_OUTPUT_DIR = self.original_paths["CHUNK_OUTPUT_DIR"]
            config.NER_OUTPUT_DIR = self.original_paths["NER_OUTPUT_DIR"]
            config.NER_PRO_OUTPUT_DIR = self.original_paths["NER_PRO_OUTPUT_DIR"]
            config.RE_OUTPUT_DIR = self.original_paths["RE_OUTPUT_DIR"]
    
    async def process_document(self, file_path: str, filename: str, 
                             target_graph_id: str = None,
                             progress_callback: Optional[Callable[[int, str], None]] = None) -> Dict[str, Any]:
        """å¤„ç†æ–‡æ¡£å¹¶é™„åŠ åˆ°ç°æœ‰çŸ¥è¯†å›¾è°±
        
        Args:
            file_path: æ–‡æ¡£æ–‡ä»¶è·¯å¾„
            filename: æ–‡æ¡£æ–‡ä»¶å
            target_graph_id: ç›®æ ‡å›¾è°±IDï¼ˆå¯é€‰ï¼Œå¦‚æœä¸æŒ‡å®šåˆ™åˆ›å»ºæ–°å›¾è°±ï¼‰
            progress_callback: è¿›åº¦å›è°ƒå‡½æ•°
        
        Returns:
            Dict: å¤„ç†ç»“æœï¼ŒåŒ…å«æˆåŠŸçŠ¶æ€ã€å›¾è°±IDã€ç»Ÿè®¡ä¿¡æ¯ç­‰
        """
        graph_id = target_graph_id
        
        try:
            # è®°å½•æ•´ä¸ªæµç¨‹çš„å¼€å§‹æ—¶é—´
            total_start_time = datetime.now()
            
            print(f"ğŸš€ å¼€å§‹å¤„ç†æ–‡æ¡£: {filename}")
            print(f"ğŸ“ æ–‡ä»¶è·¯å¾„: {file_path}")
            print(f"ğŸ”§ æ„å»ºæ¨¡å¼: é™„åŠ åˆ°ç°æœ‰å›¾è°±")
            print(f"ğŸ¯ ç›®æ ‡å›¾è°±ID: {target_graph_id}")
            
            # ğŸ†• è·å–å›¾è°±ä¿¡æ¯ä»¥ç¡®å®šä½¿ç”¨çš„ç›®å½•
            if not target_graph_id:
                raise ValueError("ç›®æ ‡å›¾è°±IDä¸èƒ½ä¸ºç©º")
                
            graph_info = self.data_manager.get_graph(target_graph_id)
            if not graph_info:
                raise ValueError(f"å›¾è°± {target_graph_id} ä¸å­˜åœ¨")
                
            graph_name = graph_info.get('name')
            print(f"ğŸ“Š ç›®æ ‡å›¾è°±åç§°: {graph_name}")
            
            # è®¾ç½®å·¥ä½œç›®å½• - ä½¿ç”¨å›¾è°±IDè€Œä¸æ˜¯å›¾è°±åç§°
            work_dirs = self._get_work_directories(target_graph_id)
            self._update_config_paths(work_dirs)
            print(f"ğŸ“‚ ä½¿ç”¨æ•°æ®ç›®å½•: {work_dirs}")
            
            if progress_callback:
                progress_callback(5, "å¼€å§‹æ–‡æ¡£é¢„å¤„ç†")
            
            # 1. æ–‡æ¡£é¢„å¤„ç†
            print("ğŸ“ æ­¥éª¤1: å¼€å§‹æ–‡æ¡£é¢„å¤„ç†...")
            processed_file_path = await self._preprocess_document(file_path, filename)
            print(f"âœ… æ–‡æ¡£é¢„å¤„ç†å®Œæˆï¼Œè¾“å‡ºæ–‡ä»¶: {processed_file_path}")
            if progress_callback:
                progress_callback(15, "æ–‡æ¡£é¢„å¤„ç†å®Œæˆ")
            
            # 2. æ–‡æœ¬åˆ†å—
            print("ğŸ”ª æ­¥éª¤2: å¼€å§‹æ–‡æ¡£åˆ†å—...")
            chunk_result = await self._chunk_document(processed_file_path)
            print(f"âœ… æ–‡æ¡£åˆ†å—å®Œæˆï¼Œç”Ÿæˆ {chunk_result.get('chunks_count', 0)} ä¸ªå—")
            if progress_callback:
                progress_callback(30, f"æ–‡æœ¬åˆ†å—å®Œæˆï¼Œå…±ç”Ÿæˆ {chunk_result.get('chunks_count', 0)} ä¸ªæ–‡æœ¬å—")
            
            # 3. å®ä½“è¯†åˆ«
            print("ğŸ·ï¸ æ­¥éª¤3: å¼€å§‹å®ä½“è¯†åˆ«...")
            ner_result = await self._extract_entities(chunk_result["chunk_file"])
            print(f"âœ… å®ä½“è¯†åˆ«å®Œæˆï¼Œè¯†åˆ«å‡º {ner_result.get('entities_count', 0)} ä¸ªå®ä½“")
            if progress_callback:
                progress_callback(45, f"å®ä½“è¯†åˆ«å®Œæˆï¼Œå…±è¯†åˆ« {ner_result.get('entities_count', 0)} ä¸ªå®ä½“")
            
            # 4. å®ä½“æ¶ˆæ­§
            print("ğŸ” æ­¥éª¤4: å¼€å§‹å®ä½“æ¶ˆæ­§...")
            disambig_result = await self._disambiguate_entities()
            print(f"âœ… å®ä½“æ¶ˆæ­§å®Œæˆï¼Œå¤„ç† {disambig_result.get('entities_count', 0)} ä¸ªå”¯ä¸€å®ä½“")
            if progress_callback:
                progress_callback(55, f"å®ä½“æ¶ˆæ­§å®Œæˆï¼Œå…±å¤„ç† {disambig_result.get('entities_count', 0)} ä¸ªå”¯ä¸€å®ä½“")
            
            # 5. å…³ç³»æŠ½å–
            print("ğŸ”— æ­¥éª¤5: å¼€å§‹å…³ç³»æŠ½å–...")
            relation_result = await self._extract_relations(progress_callback)
            print(f"âœ… å…³ç³»æŠ½å–å®Œæˆï¼ŒæŠ½å–å‡º {relation_result.get('relations_count', 0)} ä¸ªå…³ç³»")
            
            # 6. æ„å»ºçŸ¥è¯†å›¾è°±
            print("ğŸ•¸ï¸ æ­¥éª¤6: å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            kg_result = await self._build_knowledge_graph(filename, ner_result, relation_result, graph_id, progress_callback)
            print(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼Œå›¾è°±ID: {kg_result['graph_id']}")
            
            # 7. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            print("ğŸ§¹ æ­¥éª¤7: æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
            await self._cleanup_temp_files(file_path)
            print("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")
            
            print("ğŸ‰ çŸ¥è¯†å›¾è°±æ„å»ºæµç¨‹å…¨éƒ¨å®Œæˆï¼")
            
            # æ¢å¤åŸå§‹é…ç½®è·¯å¾„
            self._restore_config_paths()
            
            # è®¡ç®—æ•´ä¸ªæµç¨‹çš„æ€»å¤„ç†æ—¶é—´
            total_end_time = datetime.now()
            total_processing_time = (total_end_time - total_start_time).total_seconds()
            print(f"â±ï¸ æ—¶é—´è®¡ç®—è°ƒè¯•ä¿¡æ¯:")
            print(f"   å¼€å§‹æ—¶é—´: {total_start_time}")
            print(f"   ç»“æŸæ—¶é—´: {total_end_time}")
            print(f"   æ€»å¤„ç†æ—¶é—´: {total_processing_time:.2f}ç§’")
            
            # ä½¿ç”¨æ„å»ºç»“æœä¸­çš„å›¾è°±ID
            final_graph_id = kg_result["graph_id"]
            
            return {
                "success": True,
                "graph_id": final_graph_id,
                "statistics": {
                    "entities_count": kg_result["entities_count"],
                    "relations_count": kg_result["relations_count"],
                    "chunks_processed": chunk_result.get("chunks_count", 0),
                    "processing_time": f"{total_processing_time:.2f}ç§’"
                },
                "message": "çŸ¥è¯†å›¾è°±æ„å»ºæˆåŠŸ",
                "details": {
                    "chunk_result": chunk_result,
                    "ner_result": ner_result,
                    "disambig_result": disambig_result,
                    "relation_result": relation_result,
                    "kg_result": kg_result
                }
            }
            
        except Exception as e:
            print(f"ğŸ’¥ å¤„ç†æ–‡æ¡£æ—¶å‘ç”Ÿå¼‚å¸¸: {str(e)}")
            import traceback
            print(f"ğŸ“‹ å¼‚å¸¸å †æ ˆ: {traceback.format_exc()}")
            
            # æ¢å¤åŸå§‹é…ç½®è·¯å¾„
            self._restore_config_paths()
            
            if progress_callback:
                progress_callback(0, f"å¤„ç†å¤±è´¥: {str(e)}")
            
            # å°è¯•æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            try:
                await self._cleanup_temp_files(file_path)
            except:
                pass
            
            return {
                "success": False,
                "error": str(e),
                "message": "çŸ¥è¯†å›¾è°±æ„å»ºå¤±è´¥"
            }
    
    async def _preprocess_document(self, file_path: str, filename: str) -> str:
        """æ–‡æ¡£é¢„å¤„ç† - è½¬æ¢ä¸ºmarkdownæ ¼å¼"""
        try:
            print(f"ğŸ“ å¼€å§‹æ–‡æ¡£é¢„å¤„ç†ï¼Œå¤„ç†æ–‡ä»¶: {file_path}")
            print(f"ğŸ“„ æ–‡ä»¶å: {filename}")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(file_path):
                print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                raise Exception(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # ç¡®ä¿é¢„å¤„ç†è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(config.PROCESSED_TEXT_DIR, exist_ok=True)
            print(f"ğŸ“ é¢„å¤„ç†è¾“å‡ºç›®å½•: {config.PROCESSED_TEXT_DIR}")
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            print("ğŸ“– è¯»å–æ–‡ä»¶å†…å®¹...")
            try:
                with open(file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"ğŸ“Š åŸå§‹æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
                print(f"ğŸ“ åŸå§‹å†…å®¹é¢„è§ˆ: {content[:200]}...")
            except Exception as e:
                print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
                raise Exception(f"è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
            
            # ç®€å•çš„æ–‡æœ¬é¢„å¤„ç†
            print("ğŸ”„ å¼€å§‹æ–‡æœ¬æ¸…ç†...")
            processed_content = self._clean_text(content)
            print(f"ğŸ“Š å¤„ç†åæ–‡ä»¶å¤§å°: {len(processed_content)} å­—ç¬¦")
            print(f"ğŸ“ å¤„ç†åå†…å®¹é¢„è§ˆ: {processed_content[:200]}...")
            
            # ä¿å­˜åˆ°processed_textç›®å½•
            processed_filename = filename.replace('.txt', '.md').replace('.pdf', '.md')
            processed_file_path = os.path.join(config.PROCESSED_TEXT_DIR, processed_filename)
            print(f"ğŸ’¾ ä¿å­˜é¢„å¤„ç†ç»“æœåˆ°: {processed_file_path}")
            
            try:
                with open(processed_file_path, 'w', encoding='utf-8') as f:
                    f.write(processed_content)
                print("âœ… é¢„å¤„ç†ç»“æœä¿å­˜æˆåŠŸ")
            except Exception as e:
                print(f"âŒ ä¿å­˜é¢„å¤„ç†ç»“æœå¤±è´¥: {e}")
                raise Exception(f"ä¿å­˜é¢„å¤„ç†ç»“æœå¤±è´¥: {e}")
            
            return processed_file_path
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£é¢„å¤„ç†è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise Exception(f"æ–‡æ¡£é¢„å¤„ç†å¤±è´¥: {e}")
    
    def _clean_text(self, text: str) -> str:
        """æ¸…ç†æ–‡æœ¬å†…å®¹"""
        print("ğŸ§¹ å¼€å§‹æ¸…ç†æ–‡æœ¬å†…å®¹...")
        
        # ç§»é™¤å¤šä½™çš„ç©ºç™½å­—ç¬¦
        lines = text.split('\n')
        cleaned_lines = []
        
        original_lines_count = len(lines)
        print(f"ğŸ“Š åŸå§‹è¡Œæ•°: {original_lines_count}")
        
        for line in lines:
            line = line.strip()
            if line:  # è·³è¿‡ç©ºè¡Œ
                cleaned_lines.append(line)
        
        cleaned_lines_count = len(cleaned_lines)
        print(f"ğŸ“Š æ¸…ç†åè¡Œæ•°: {cleaned_lines_count} (ç§»é™¤äº† {original_lines_count - cleaned_lines_count} ä¸ªç©ºè¡Œ)")
        
        result = '\n'.join(cleaned_lines)
        print(f"âœ… æ–‡æœ¬æ¸…ç†å®Œæˆ")
        
        return result
    
    async def _chunk_document(self, processed_file_path: str) -> Dict[str, Any]:
        """æ–‡æ¡£åˆ†å—å¤„ç†"""
        try:
            print(f"ğŸ”ª å¼€å§‹æ–‡æ¡£åˆ†å—ï¼Œå¤„ç†æ–‡ä»¶: {processed_file_path}")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(processed_file_path):
                print(f"âŒ é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨: {processed_file_path}")
                raise Exception(f"é¢„å¤„ç†æ–‡ä»¶ä¸å­˜åœ¨: {processed_file_path}")
            
            # ç¡®ä¿åˆ†å—è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(config.CHUNK_OUTPUT_DIR, exist_ok=True)
            print(f"ğŸ“ åˆ†å—è¾“å‡ºç›®å½•: {config.CHUNK_OUTPUT_DIR}")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å†…å®¹
            try:
                with open(processed_file_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                print(f"ğŸ“Š è¾“å…¥æ–‡ä»¶å¤§å°: {len(content)} å­—ç¬¦")
                print(f"ğŸ“ æ–‡ä»¶å†…å®¹é¢„è§ˆ: {content[:200]}...")
            except Exception as e:
                print(f"âŒ è¯»å–é¢„å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
                raise Exception(f"è¯»å–é¢„å¤„ç†æ–‡ä»¶å¤±è´¥: {e}")
            
            print("ğŸ”„ å¼€å§‹è°ƒç”¨åˆ†å—å¤„ç†å‡½æ•°...")
            # è°ƒç”¨ç°æœ‰çš„åˆ†å—å‡½æ•°
            await asyncio.get_event_loop().run_in_executor(
                None, run_chunk_on_file, processed_file_path
            )
            print("âœ… åˆ†å—å¤„ç†å‡½æ•°è°ƒç”¨å®Œæˆ")
            
            # è·å–åˆ†å—ç»“æœæ–‡ä»¶è·¯å¾„
            filename = os.path.basename(processed_file_path)
            chunk_filename = filename.replace('.md', '.json')
            chunk_file_path = os.path.join(config.CHUNK_OUTPUT_DIR, chunk_filename)
            
            print(f"ğŸ” æ£€æŸ¥åˆ†å—ç»“æœæ–‡ä»¶: {chunk_file_path}")
            
            # æ£€æŸ¥åˆ†å—ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(chunk_file_path):
                print(f"âŒ åˆ†å—ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {chunk_file_path}")
                # åˆ—å‡ºåˆ†å—è¾“å‡ºç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                try:
                    files_in_dir = os.listdir(config.CHUNK_OUTPUT_DIR)
                    print(f"ğŸ“‚ åˆ†å—è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶: {files_in_dir}")
                except Exception as e:
                    print(f"âŒ æ— æ³•åˆ—å‡ºåˆ†å—è¾“å‡ºç›®å½•æ–‡ä»¶: {e}")
                raise Exception("åˆ†å—ç»“æœæ–‡ä»¶æœªç”Ÿæˆ")
            
            # è¯»å–åˆ†å—ç»“æœ
            print("ğŸ“– è¯»å–åˆ†å—ç»“æœæ–‡ä»¶...")
            chunks = load_json(chunk_file_path)
            chunks_count = len(chunks) if chunks else 0
            print(f"âœ… æˆåŠŸè¯»å–åˆ†å—ç»“æœï¼Œå…± {chunks_count} ä¸ªæ–‡æœ¬å—")
            
            if chunks and len(chunks) > 0:
                chunk_keys = list(chunks.keys())
                print(f"ğŸ“ åˆ†å—ç¤ºä¾‹: {chunk_keys[0] if chunk_keys else 'None'}")
            
            return {
                "success": True,
                "chunk_file": chunk_file_path,
                "chunks_count": chunks_count
            }
            
        except Exception as e:
            print(f"âŒ æ–‡æ¡£åˆ†å—è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise Exception(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
    
    async def _extract_entities(self, chunk_file_path: str) -> Dict[str, Any]:
        """å®ä½“è¯†åˆ«å¤„ç†"""
        try:
            print(f"ğŸ”„ å¼€å§‹å®ä½“è¯†åˆ«ï¼Œå¤„ç†æ–‡ä»¶: {chunk_file_path}")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(chunk_file_path):
                print(f"âŒ è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {chunk_file_path}")
                raise Exception(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨: {chunk_file_path}")
            
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(config.NER_OUTPUT_DIR, exist_ok=True)
            print(f"ğŸ“ NERè¾“å‡ºç›®å½•: {config.NER_OUTPUT_DIR}")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶å†…å®¹
            try:
                chunks = load_json(chunk_file_path)
                print(f"ğŸ“Š è¯»å–åˆ° {len(chunks) if chunks else 0} ä¸ªæ–‡æœ¬å—")
                if chunks:
                    print(f"ğŸ“ ç¬¬ä¸€ä¸ªå—é¢„è§ˆ: {str(list(chunks.keys())[0] if chunks else 'None')[:100]}...")
            except Exception as e:
                print(f"âŒ è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
                raise Exception(f"è¯»å–è¾“å…¥æ–‡ä»¶å¤±è´¥: {e}")
            
            print("ğŸ”„ å¼€å§‹è°ƒç”¨NERå¤„ç†å‡½æ•°...")
            # è°ƒç”¨ç°æœ‰çš„NERå‡½æ•°
            # è°ƒç”¨step2_ner.pyä¸­çš„run_ner_on_fileå‡½æ•°è¿›è¡Œå®ä½“è¯†åˆ«
            # å‚æ•°1: chunk_file_path - åˆ†å—åçš„æ–‡ä»¶è·¯å¾„
            # å‚æ•°2: 2 - ä½¿ç”¨2ä¸ªçº¿ç¨‹è¿›è¡Œå¹¶è¡Œå¤„ç†
            await asyncio.get_event_loop().run_in_executor(
                None, run_ner_on_file, chunk_file_path, 2
            )
            print("âœ… NERå¤„ç†å‡½æ•°è°ƒç”¨å®Œæˆ")
            
            # è·å–NERç»“æœæ–‡ä»¶è·¯å¾„
            filename = os.path.basename(chunk_file_path)
            ner_file_path = os.path.join(config.NER_OUTPUT_DIR, filename)
            
            print(f"ğŸ“ æŸ¥æ‰¾NERç»“æœæ–‡ä»¶: {ner_file_path}")
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(ner_file_path):
                print(f"âš ï¸ NERç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {ner_file_path}")
                # åˆ—å‡ºè¾“å‡ºç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                try:
                    files_in_dir = os.listdir(config.NER_OUTPUT_DIR)
                    print(f"ğŸ“‚ NERè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶: {files_in_dir}")
                except Exception as e:
                    print(f"âŒ æ— æ³•åˆ—å‡ºNERè¾“å‡ºç›®å½•æ–‡ä»¶: {e}")
                
                return {
                    "success": False,
                    "error": "NERç»“æœæ–‡ä»¶æœªç”Ÿæˆ",
                    "entities_count": 0
                }
            
            # è¯»å–NERç»“æœ
            print("ğŸ“– è¯»å–NERç»“æœæ–‡ä»¶...")
            entities = load_json(ner_file_path)
            print(f"âœ… æˆåŠŸè¯»å–NERç»“æœï¼Œå…± {len(entities) if entities else 0} ä¸ªå®ä½“")
            
            if entities and len(entities) > 0:
                print(f"ğŸ“ å®ä½“ç¤ºä¾‹: {entities[0] if entities else 'None'}")
            
            return {
                "success": True,
                "ner_file": ner_file_path,
                "entities_count": len(entities) if entities else 0
            }
            
        except Exception as e:
            print(f"âŒ å®ä½“è¯†åˆ«è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise Exception(f"å®ä½“è¯†åˆ«å¤±è´¥: {e}")
    
    async def _disambiguate_entities(self) -> Dict[str, Any]:
        """å®ä½“æ¶ˆæ­§å¤„ç†"""
        try:
            print("ğŸ” å¼€å§‹å®ä½“æ¶ˆæ­§å¤„ç†...")
            
            # æ£€æŸ¥è¾“å…¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            ner_output_dir = config.NER_OUTPUT_DIR
            print(f"ğŸ“ æ£€æŸ¥NERè¾“å‡ºç›®å½•: {ner_output_dir}")
            
            if not os.path.exists(ner_output_dir):
                print(f"âŒ NERè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {ner_output_dir}")
                raise Exception(f"NERè¾“å‡ºç›®å½•ä¸å­˜åœ¨: {ner_output_dir}")
            
            # åˆ—å‡ºNERè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶
            try:
                ner_files = os.listdir(ner_output_dir)
                print(f"ğŸ“‚ NERè¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶: {ner_files}")
            except Exception as e:
                print(f"âŒ æ— æ³•åˆ—å‡ºNERè¾“å‡ºç›®å½•æ–‡ä»¶: {e}")
                raise Exception(f"æ— æ³•è®¿é—®NERè¾“å‡ºç›®å½•: {e}")
            
            # ç¡®ä¿æ¶ˆæ­§è¾“å‡ºç›®å½•å­˜åœ¨
            os.makedirs(config.NER_PRO_OUTPUT_DIR, exist_ok=True)
            print(f"ğŸ“ æ¶ˆæ­§è¾“å‡ºç›®å½•: {config.NER_PRO_OUTPUT_DIR}")
            
            print("ğŸ”„ å¼€å§‹è°ƒç”¨æ¶ˆæ­§å¤„ç†å‡½æ•°...")
            # ç›´æ¥è°ƒç”¨æ¶ˆæ­§å¤„ç†å‡½æ•°
            success = await asyncio.get_event_loop().run_in_executor(
                None, run_disambiguate_on_all_files
            )
            print(f"âœ… æ¶ˆæ­§å¤„ç†å‡½æ•°è°ƒç”¨å®Œæˆï¼Œç»“æœ: {success}")
            
            if not success:
                print("âŒ å®ä½“æ¶ˆæ­§å¤„ç†è¿”å›å¤±è´¥")
                raise Exception("å®ä½“æ¶ˆæ­§å¤„ç†å¤±è´¥")
            
            # è·å–æ¶ˆæ­§ç»“æœæ–‡ä»¶è·¯å¾„
            disambig_file_path = os.path.join(config.NER_PRO_OUTPUT_DIR, "all_entities_disambiguated.json")
            print(f"ğŸ” æ£€æŸ¥æ¶ˆæ­§ç»“æœæ–‡ä»¶: {disambig_file_path}")
            
            # æ£€æŸ¥æ¶ˆæ­§ç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(disambig_file_path):
                print(f"âŒ æ¶ˆæ­§ç»“æœæ–‡ä»¶ä¸å­˜åœ¨: {disambig_file_path}")
                # åˆ—å‡ºæ¶ˆæ­§è¾“å‡ºç›®å½•çš„æ‰€æœ‰æ–‡ä»¶
                try:
                    files_in_dir = os.listdir(config.NER_PRO_OUTPUT_DIR)
                    print(f"ğŸ“‚ æ¶ˆæ­§è¾“å‡ºç›®å½•ä¸­çš„æ–‡ä»¶: {files_in_dir}")
                except Exception as e:
                    print(f"âŒ æ— æ³•åˆ—å‡ºæ¶ˆæ­§è¾“å‡ºç›®å½•æ–‡ä»¶: {e}")
                raise Exception("æ¶ˆæ­§ç»“æœæ–‡ä»¶æœªç”Ÿæˆ")
            
            # è¯»å–æ¶ˆæ­§ç»“æœ
            print("ğŸ“– è¯»å–æ¶ˆæ­§ç»“æœæ–‡ä»¶...")
            disambiguated_entities = load_json(disambig_file_path)
            entities_count = len(disambiguated_entities) if disambiguated_entities else 0
            print(f"âœ… æˆåŠŸè¯»å–æ¶ˆæ­§ç»“æœï¼Œå…± {entities_count} ä¸ªå®ä½“")
            
            if disambiguated_entities and len(disambiguated_entities) > 0:
                print(f"ğŸ“ æ¶ˆæ­§å®ä½“ç¤ºä¾‹: {disambiguated_entities[0] if disambiguated_entities else 'None'}")
            
            return {
                "success": True,
                "disambig_file": disambig_file_path,
                "entities_count": entities_count
            }
            
        except Exception as e:
            print(f"âŒ å®ä½“æ¶ˆæ­§è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            print(f"ğŸ“‹ é”™è¯¯å †æ ˆ: {traceback.format_exc()}")
            raise Exception(f"å®ä½“æ¶ˆæ­§å¤±è´¥: {e}")
    
    async def _extract_relations(self, progress_callback: Optional[Callable[[int, str], None]] = None) -> Dict[str, Any]:
        """å…³ç³»æŠ½å–å¤„ç†"""
        try:
            if progress_callback:
                progress_callback(60, "å¼€å§‹å…³ç³»æŠ½å–...")
            
            # è°ƒç”¨å¸¦è¿›åº¦çš„å…³ç³»æŠ½å–å‡½æ•°
            await asyncio.get_event_loop().run_in_executor(
                None, self._run_relation_extraction_with_progress, progress_callback
            )
            
            # è·å–å…³ç³»æŠ½å–ç»“æœæ–‡ä»¶è·¯å¾„
            relations_file_path = os.path.join(config.RE_OUTPUT_DIR, "all_relations.json")
            
            # è¯»å–å…³ç³»æŠ½å–ç»“æœ
            relations = load_json(relations_file_path)
            
            if progress_callback:
                progress_callback(80, f"å…³ç³»æŠ½å–å®Œæˆï¼Œå…±æŠ½å– {len(relations) if relations else 0} ä¸ªå…³ç³»")
            
            return {
                "success": True,
                "relations_file": relations_file_path,
                "relations_count": len(relations) if relations else 0
            }
            
        except Exception as e:
            raise Exception(f"å…³ç³»æŠ½å–å¤±è´¥: {e}")
    
    def _run_relation_extraction_with_progress(self, progress_callback: Optional[Callable[[int, str], None]] = None):
        """å¸¦è¿›åº¦æ˜¾ç¤ºçš„å…³ç³»æŠ½å–"""
        print("ğŸ”„ å¼€å§‹å…³ç³»æŠ½å–å¤„ç†...")
        
        # åŠ è½½æ¶ˆæ­§åçš„å®ä½“æ–‡ä»¶
        disambiguated_entities_path = os.path.join(config.NER_PRO_OUTPUT_DIR, "all_entities_disambiguated.json")
        
        if not os.path.exists(disambiguated_entities_path):
            raise Exception(f"æ‰¾ä¸åˆ°æ¶ˆæ­§å®ä½“æ–‡ä»¶: {disambiguated_entities_path}")
        
        print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ¶ˆæ­§å®ä½“æ–‡ä»¶: all_entities_disambiguated.json")
        disambiguated_entities = load_json(disambiguated_entities_path)
        
        if not disambiguated_entities:
            raise Exception("æ¶ˆæ­§å®ä½“æ•°æ®ä¸ºç©º")
        
        print(f"âœ… åŠ è½½äº† {len(disambiguated_entities)} ä¸ªæ¶ˆæ­§åçš„å®ä½“")
        
        # åŠ è½½æ‰€æœ‰æ–‡æ¡£chunksï¼ˆé™„åŠ æ¨¡å¼ï¼‰
        print("ğŸ“š åŠ è½½æ‰€æœ‰æ–‡æ¡£chunks...")
        all_chunks = self._load_all_chunks()
        
        if not all_chunks:
            raise Exception("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•chunkæ•°æ®")
        
        # åˆ›å»º chunk_id -> entities çš„æ˜ å°„
        chunk_to_entities_map = self._create_chunk_to_entities_map(disambiguated_entities)
        
        extracted_triples = []
        total_chunks = len(chunk_to_entities_map)
        
        print(f"ğŸ”— å¼€å§‹å¤„ç† {total_chunks} ä¸ªæ–‡æœ¬å—çš„å…³ç³»æŠ½å–")
        
        for i, (chunk_id, entities) in enumerate(chunk_to_entities_map.items()):
            # æ›´æ–°è¿›åº¦
            progress = 60 + int((i / total_chunks) * 15)  # 60-75%çš„è¿›åº¦èŒƒå›´
            if progress_callback:
                progress_callback(progress, f"æ­£åœ¨å¤„ç†ç¬¬ {i+1}/{total_chunks} ä¸ªæ–‡æœ¬å—: {chunk_id[:20]}...")
            
            chunk_text = all_chunks.get(chunk_id, "")
            if not chunk_text:
                print(f"âš ï¸  Chunk {chunk_id} åœ¨chunkæ•°æ®ä¸­ä¸å­˜åœ¨ï¼Œè·³è¿‡")
                continue
            
            if len(entities) < 2:
                continue  # å®ä½“æ•°é‡å°‘äº2ä¸ªï¼Œæ— æ³•å½¢æˆå…³ç³»
            
            # ä¸ºå½“å‰å—æŠ½å–å…³ç³»
            triples_from_chunk = self._extract_relations_for_chunk(chunk_id, chunk_text, entities)
            
            validated_triples = []
            # å°†å®ä½“åˆ—è¡¨è½¬ä¸ºé›†åˆï¼Œä¾¿äºå¿«é€ŸæŸ¥æ‰¾
            valid_entity_set = set(entities)
            
            for triple in triples_from_chunk:
                if len(triple) == 3:
                    head, relation, tail = triple
                    # æ£€æŸ¥å¤´å®ä½“å’Œå°¾å®ä½“æ˜¯å¦éƒ½åœ¨åˆæ³•çš„å®ä½“åˆ—è¡¨ä¸­
                    if head in valid_entity_set and tail in valid_entity_set:
                        validated_triples.append(triple)
                    else:
                        print(f"--- [å·²è¿‡æ»¤] å‘ç°å¹»è§‰å®ä½“ï¼Œå·²ä¸¢å¼ƒ: {triple}")
            
            # ä¸ºæ¯ä¸ªä¸‰å…ƒç»„æ·»åŠ æ¥æºä¿¡æ¯
            for triple in validated_triples:
                if len(triple) == 3:
                    extracted_triples.append({
                        "head": triple[0],
                        "relation": triple[1],
                        "tail": triple[2],
                        "source_chunk_id": chunk_id
                    })
        
        # å»é‡å¤„ç†
        if progress_callback:
            progress_callback(75, "æ­£åœ¨å»é‡å’Œæ•´ç†å…³ç³»æ•°æ®...")
        
        unique_triples_str = {json.dumps(d, sort_keys=True) for d in extracted_triples}
        final_triples = [json.loads(s) for s in unique_triples_str]
        
        # ä¿å­˜æ‰€æœ‰å…³ç³»æŠ½å–ç»“æœåˆ°ç»Ÿä¸€æ–‡ä»¶
        output_path = os.path.join(config.RE_OUTPUT_DIR, "all_relations.json")
        save_json(final_triples, output_path)
        
        print(f"âœ… å…³ç³»æŠ½å–å¤„ç†å®Œæˆï¼")
        print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
        print(f"   ğŸ·ï¸  å¤„ç†å®ä½“æ•°: {len(disambiguated_entities)}")
        print(f"   ğŸ“„ å¤„ç†chunkæ•°: {len(chunk_to_entities_map)}")
        print(f"   ğŸ”— æŠ½å–å…³ç³»æ•°: {len(final_triples)}")
        print(f"   ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_path}")
    
    def _load_all_chunks(self):
        """åŠ è½½æ‰€æœ‰chunkæ–‡ä»¶çš„å†…å®¹ï¼ˆç”¨äºé™„åŠ æ¨¡å¼ï¼‰"""
        all_chunks = {}
        
        for filename in os.listdir(config.CHUNK_OUTPUT_DIR):
            if filename.endswith(".json"):
                chunk_path = os.path.join(config.CHUNK_OUTPUT_DIR, filename)
                file_prefix = filename.replace('.json', '')
                
                chunks = load_json(chunk_path)
                if chunks:
                    # ä¸ºæ¯ä¸ªchunk_idæ·»åŠ æ–‡ä»¶åå‰ç¼€
                    for chunk_id, chunk_text in chunks.items():
                        prefixed_chunk_id = f"{file_prefix}_{chunk_id}"
                        all_chunks[prefixed_chunk_id] = chunk_text
                        
                    print(f"ğŸ“„ åŠ è½½äº†æ–‡ä»¶ {filename}ï¼ŒåŒ…å« {len(chunks)} ä¸ªchunks")
        
        print(f"ğŸ“Š æ€»å…±åŠ è½½äº† {len(all_chunks)} ä¸ªchunks")
        return all_chunks
    
    def _load_current_chunks(self):
        """ä»…åŠ è½½å½“å‰æ–‡æ¡£çš„chunkæ–‡ä»¶å†…å®¹ï¼ˆç”¨äºç‹¬ç«‹æ„å»ºæ¨¡å¼ï¼‰"""
        all_chunks = {}
        
        # è·å–æœ€æ–°çš„chunkæ–‡ä»¶ï¼ˆæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼‰
        chunk_files = []
        for filename in os.listdir(config.CHUNK_OUTPUT_DIR):
            if filename.endswith(".json"):
                chunk_path = os.path.join(config.CHUNK_OUTPUT_DIR, filename)
                mtime = os.path.getmtime(chunk_path)
                chunk_files.append((mtime, filename, chunk_path))
        
        if not chunk_files:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•chunkæ–‡ä»¶")
            return all_chunks
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè·å–æœ€æ–°çš„æ–‡ä»¶
        chunk_files.sort(reverse=True)
        latest_file = chunk_files[0]
        filename = latest_file[1]
        chunk_path = latest_file[2]
        
        print(f"ğŸ“„ ç‹¬ç«‹æ„å»ºæ¨¡å¼ï¼šä»…åŠ è½½æœ€æ–°çš„chunkæ–‡ä»¶ {filename}")
        
        file_prefix = filename.replace('.json', '')
        chunks = load_json(chunk_path)
        if chunks:
            # ä¸ºæ¯ä¸ªchunk_idæ·»åŠ æ–‡ä»¶åå‰ç¼€
            for chunk_id, chunk_text in chunks.items():
                prefixed_chunk_id = f"{file_prefix}_{chunk_id}"
                all_chunks[prefixed_chunk_id] = chunk_text
                
            print(f"ğŸ“„ åŠ è½½äº†æ–‡ä»¶ {filename}ï¼ŒåŒ…å« {len(chunks)} ä¸ªchunks")
        
        print(f"ğŸ“Š ç‹¬ç«‹æ„å»ºæ¨¡å¼ï¼šæ€»å…±åŠ è½½äº† {len(all_chunks)} ä¸ªchunks")
        return all_chunks
    
    def _create_chunk_to_entities_map(self, disambiguated_entities: list):
        """åˆ›å»ºä»chunk_idåˆ°å®ä½“åç§°åˆ—è¡¨çš„æ˜ å°„"""
        from collections import defaultdict
        chunk_map = defaultdict(list)
        for entity in disambiguated_entities:
            # æˆ‘ä»¬åªå…³å¿ƒè§„èŒƒåç§°
            canonical_name = entity["entity_text"]
            for chunk_id in entity["chunk_id"]:
                if canonical_name not in chunk_map[chunk_id]:
                    chunk_map[chunk_id].append(canonical_name)
        return chunk_map
    
    def _extract_relations_for_chunk(self, chunk_id: str, chunk_text: str, entities_in_chunk: list):
        """ä¸ºå•ä¸ªæ–‡æœ¬å—è°ƒç”¨LLMè¿›è¡Œå…³ç³»æŠ½å–"""
        try:
            # åŠ è½½å…³ç³»æŠ½å–æç¤ºè¯
            re_prompt = load_prompt(config.RE_PROMPT_PATH)
            
            # æ„å»ºå®ä½“åˆ—è¡¨å­—ç¬¦ä¸²
            entities_str = ", ".join(entities_in_chunk)
            
            # åŠ è½½å…³ç³»ç±»å‹
            relation_types = "\n".join([
                "ä½¿ç”¨åŸæ–™", "ç”Ÿäº§äº§å“", "åº”ç”¨äº", "å…·æœ‰æ€§è´¨", "åŒ…å«æˆåˆ†",
                "ç»è¿‡å·¥è‰º", "ä½¿ç”¨è®¾å¤‡", "äº§ç”Ÿå‰¯äº§å“", "å½±å“å› ç´ ", "æµ‹é‡æŒ‡æ ‡"
            ])
            
            # æ„å»ºå®Œæ•´çš„æç¤ºè¯
            full_prompt = re_prompt.replace("{{CHUNK_TEXT}}", chunk_text).replace("{{ENTITIES_IN_CHUNK}}", entities_str).replace("{{RELATION_TYPES}}", relation_types)
            
            # è°ƒç”¨LLM
            response = call_llm(full_prompt)
            
            # è§£æå“åº”ï¼Œæå–ä¸‰å…ƒç»„
            if isinstance(response, list):
                # å¦‚æœè¿”å›çš„æ˜¯åˆ—è¡¨ï¼Œç›´æ¥å¤„ç†
                triples = []
                for item in response:
                    if isinstance(item, dict) and 'head' in item and 'relation' in item and 'tail' in item:
                        triples.append([item['head'], item['relation'], item['tail']])
                    elif isinstance(item, list) and len(item) == 3:
                        triples.append(item)
            elif isinstance(response, dict) and 'relations' in response:
                # å¦‚æœè¿”å›çš„æ˜¯åŒ…å«relationså­—æ®µçš„å­—å…¸
                triples = []
                for item in response['relations']:
                    if isinstance(item, dict) and 'head' in item and 'relation' in item and 'tail' in item:
                        triples.append([item['head'], item['relation'], item['tail']])
                    elif isinstance(item, list) and len(item) == 3:
                        triples.append(item)
            else:
                # å…¶ä»–æƒ…å†µè¿”å›ç©ºåˆ—è¡¨
                triples = []
            
            print(f"ğŸ“ Chunk {chunk_id}: å‘ç° {len(triples)} ä¸ªå…³ç³»")
            return triples
            
        except Exception as e:
            print(f"âŒ å¤„ç†chunk {chunk_id} æ—¶å‡ºé”™: {e}")
            return []
    
    def _parse_relation_response(self, response: str):
        """è§£æLLMçš„å…³ç³»æŠ½å–å“åº”"""
        triples = []
        try:
            # å°è¯•è§£æJSONæ ¼å¼çš„å“åº”
            if response.strip().startswith('['):
                parsed = json.loads(response)
                for item in parsed:
                    if isinstance(item, dict) and 'head' in item and 'relation' in item and 'tail' in item:
                        triples.append([item['head'], item['relation'], item['tail']])
                    elif isinstance(item, list) and len(item) == 3:
                        triples.append(item)
            else:
                # å°è¯•è§£ææ–‡æœ¬æ ¼å¼çš„å“åº”
                lines = response.strip().split('\n')
                for line in lines:
                    line = line.strip()
                    if '|' in line:
                        parts = [p.strip() for p in line.split('|')]
                        if len(parts) == 3:
                            triples.append(parts)
                    elif '->' in line:
                        # å¤„ç† "å®ä½“1 -> å…³ç³» -> å®ä½“2" æ ¼å¼
                        parts = [p.strip() for p in line.split('->')]
                        if len(parts) == 3:
                            triples.append([parts[0], parts[1], parts[2]])
        except Exception as e:
            print(f"è§£æå…³ç³»å“åº”æ—¶å‡ºé”™: {e}")
        
        return triples
    
    async def _build_knowledge_graph(self, filename: str, ner_result: Dict, relation_result: Dict, 
                                    target_graph_id: Optional[str] = None,
                                    progress_callback: Optional[Callable[[int, str], None]] = None) -> Dict[str, Any]:
        """æ„å»ºçŸ¥è¯†å›¾è°±å¹¶ä¿å­˜åˆ°æ•°æ®ç®¡ç†å™¨"""
        try:
            start_time = datetime.now()
            
            if progress_callback:
                progress_callback(85, "å¼€å§‹æ„å»ºçŸ¥è¯†å›¾è°±...")
            
            # å†³å®šå›¾è°±ID
            if target_graph_id:
                # é™„åŠ åˆ°æŒ‡å®šçš„ç›®æ ‡å›¾è°±
                graph_id = target_graph_id
                print(f"ğŸ”— é™„åŠ åˆ°ç°æœ‰å›¾è°±ID: {graph_id}")
            else:
                # åˆ›å»ºæ–°çš„çŸ¥è¯†å›¾è°±
                graph_name = f"ä» {filename} æ„å»ºçš„çŸ¥è¯†å›¾è°±"
                graph_description = f"åŸºäºæ–‡æ¡£ {filename} è‡ªåŠ¨æ„å»ºçš„çŸ¥è¯†å›¾è°±ï¼ŒåŒ…å«å®ä½“è¯†åˆ«å’Œå…³ç³»æŠ½å–ç»“æœ"
                
                graph = self.data_manager.create_graph(
                    name=graph_name,
                    description=graph_description
                )
                graph_id = graph["id"]
                print(f"ğŸ“Š åˆ›å»ºæ–°å›¾è°±ID: {graph_id}")
            
            if progress_callback:
                progress_callback(87, "æ­£åœ¨è¯»å–å®ä½“å’Œå…³ç³»æ•°æ®...")
            
            # è¯»å–æ¶ˆæ­§åçš„å®ä½“æ•°æ®
            disambig_file_path = os.path.join(config.NER_PRO_OUTPUT_DIR, "all_entities_disambiguated.json")
            entities_raw_data = load_json(disambig_file_path)
            
            # è¯»å–å…³ç³»æ•°æ®
            relations_file_path = os.path.join(config.RE_OUTPUT_DIR, "all_relations.json")
            relations_raw_data = load_json(relations_file_path)
            
            if progress_callback:
                progress_callback(90, "æ­£åœ¨è½¬æ¢æ•°æ®æ ¼å¼...")
            
            # è½¬æ¢å®ä½“æ•°æ®æ ¼å¼ä»¥åŒ¹é…import_kg_dataçš„æœŸæœ›
            entities_data = []
            for entity in entities_raw_data:
                entities_data.append({
                    "name": entity["entity_text"],
                    "type": entity["entity_type"],
                    "description": entity.get("entity_description", ""),
                    "frequency": len(entity.get("chunk_id", [])),  # ä½¿ç”¨å‡ºç°æ¬¡æ•°ä½œä¸ºé¢‘ç‡
                    "source_chunks": entity.get("chunk_id", [])
                })
            
            # è½¬æ¢å…³ç³»æ•°æ®æ ¼å¼ä»¥åŒ¹é…import_kg_dataçš„æœŸæœ›
            relations_data = []
            for relation in relations_raw_data:
                relations_data.append({
                    "source_entity": relation["head"],
                    "target_entity": relation["tail"],
                    "relation_type": relation["relation"],
                    "confidence": 0.8,  # é»˜è®¤ç½®ä¿¡åº¦
                    "description": f"ä»æ–‡æ¡£ä¸­æŠ½å–çš„å…³ç³»: {relation['head']} {relation['relation']} {relation['tail']}",
                    "source_chunk_id": relation.get("source_chunk_id", "")
                })
            
            if progress_callback:
                progress_callback(93, f"æ­£åœ¨ä¿å­˜ {len(entities_data)} ä¸ªå®ä½“å’Œ {len(relations_data)} ä¸ªå…³ç³»åˆ°å›¾è°±ç›®å½•...")
            
            # æ¸…ç†å›¾è°±çš„æ—§æ•°æ®
            print(f"ğŸ”„ å¼€å§‹æ¸…ç†å›¾è°± {graph_id} çš„æ—§æ•°æ®...")
            self.data_manager._clear_graph_data(graph_id)
            
            # ç›´æ¥å°†æ¶ˆæ­§åçš„æ•°æ®ä¿å­˜åˆ°å›¾è°±ç‰¹å®šç›®å½•
            print(f"ğŸ“ æ•°æ®å·²ç”Ÿæˆåœ¨ ner_pro_output/{graph_id} ç›®å½•ä¸­ï¼Œæ— éœ€å¤åˆ¶...")
            
            # ç›´æ¥ä½¿ç”¨ner_pro_outputç›®å½•ä¸­çš„æ•°æ®æ–‡ä»¶
            target_entities_file = disambig_file_path  # ç›´æ¥ä½¿ç”¨åŸå§‹æ¶ˆæ­§æ–‡ä»¶
            target_relations_file = relations_file_path  # ç›´æ¥ä½¿ç”¨åŸå§‹å…³ç³»æ–‡ä»¶
            
            print(f"âœ… å®ä½“æ•°æ®ä½äº: {target_entities_file}")
            print(f"âœ… å…³ç³»æ•°æ®ä½äº: {target_relations_file}")
            print(f"ğŸ’¡ æ•°æ®ç®¡ç†å™¨å°†ç›´æ¥ä» ner_pro_output ç›®å½•è¯»å–æ•°æ®")
            
            end_time = datetime.now()
            processing_time = (end_time - start_time).total_seconds()
            
            entities_count = len(entities_data)
            relations_count = len(relations_data)
            
            print(f"âœ… çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼åŒ…å« {entities_count} ä¸ªå®ä½“å’Œ {relations_count} ä¸ªå…³ç³»")
            
            if progress_callback:
                progress_callback(100, f"çŸ¥è¯†å›¾è°±æ„å»ºå®Œæˆï¼åŒ…å« {entities_count} ä¸ªå®ä½“å’Œ {relations_count} ä¸ªå…³ç³»")
            
            return {
                "success": True,
                "graph_id": graph_id,
                "entities_count": entities_count,
                "relations_count": relations_count,
                "processing_time": processing_time,
                "entities_file": str(target_entities_file),
                "relations_file": str(target_relations_file)
            }
            
        except Exception as e:
            raise Exception(f"æ„å»ºçŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
    
    async def _cleanup_temp_files(self, original_file_path: str):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶å’Œç›®å½•"""
        try:
            import shutil
            
            # åˆ é™¤ä¸Šä¼ çš„åŸå§‹æ–‡ä»¶
            if os.path.exists(original_file_path):
                os.remove(original_file_path)
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            if os.path.exists(self.temp_dir):
                shutil.rmtree(self.temp_dir)
                print(f"å·²æ¸…ç†ä¸´æ—¶ç›®å½•: {self.temp_dir}")
            
            # æ¸…ç†å¤„ç†è¿‡ç¨‹ä¸­çš„è¾“å‡ºç›®å½•ï¼ˆå¯é€‰ï¼‰
            # æ³¨æ„ï¼šè¿™é‡Œä¸æ¸…ç†configä¸­çš„è¾“å‡ºç›®å½•ï¼Œå› ä¸ºç”¨æˆ·å¯èƒ½éœ€è¦æŸ¥çœ‹ä¸­é—´ç»“æœ
            
        except Exception as e:
            print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    
    def get_supported_formats(self) -> List[str]:
        """è·å–æ”¯æŒçš„æ–‡ä»¶æ ¼å¼"""
        return ['.txt', '.md', '.pdf', '.docx']
    
    def validate_file(self, file_path: str) -> Dict[str, Any]:
        """éªŒè¯æ–‡ä»¶æ ¼å¼å’Œå¤§å°"""
        try:
            file_path = Path(file_path)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not file_path.exists():
                return {"valid": False, "error": "æ–‡ä»¶ä¸å­˜åœ¨"}
            
            # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
            if file_path.suffix.lower() not in self.get_supported_formats():
                return {
                    "valid": False, 
                    "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼ï¼Œæ”¯æŒçš„æ ¼å¼: {', '.join(self.get_supported_formats())}"
                }
            
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆé™åˆ¶ä¸º10MBï¼‰
            file_size = file_path.stat().st_size
            max_size = 10 * 1024 * 1024  # 10MB
            
            if file_size > max_size:
                return {
                    "valid": False, 
                    "error": f"æ–‡ä»¶è¿‡å¤§ï¼Œæœ€å¤§æ”¯æŒ {max_size // (1024*1024)}MB"
                }
            
            return {
                "valid": True,
                "file_size": file_size,
                "file_format": file_path.suffix.lower()
            }
            
        except Exception as e:
            return {"valid": False, "error": f"æ–‡ä»¶éªŒè¯å¤±è´¥: {str(e)}"}