# 02_scripts/step2.5_disambiguate.py

import os
import json
from collections import defaultdict
import config
from utils import call_llm, load_json, save_json, load_prompt

def disambiguate_entities_with_llm(entity_list: list):
    """
    ä½¿ç”¨LLMå¯¹æ•´ä¸ªæ–‡æ¡£çš„å®ä½“åˆ—è¡¨è¿›è¡Œèšç±»ã€‚
    å®ƒçš„ä»»åŠ¡æ˜¯è¿”å›èšç±»çš„â€œé…æ–¹â€ï¼Œè€Œä¸æ˜¯æœ€ç»ˆç»“æœã€‚
    """
    print("æ­£åœ¨å‡†å¤‡Promptï¼Œè°ƒç”¨LLMè·å–å®ä½“èšç±»é…æ–¹...")
    
    prompt_template = load_prompt(os.path.join(config.BASE_DIR, "prompts", "disambiguation_prompt.txt"))
    
    # ä¸ºäº†è®©Promptæ›´ç®€æ´ï¼Œå¯ä»¥åªå‘é€å¿…è¦çš„å­—æ®µç»™LLM
    simplified_entities = [
        {"entity_text": e["entity_text"], "entity_description": e["entity_description"]} 
        for e in entity_list
    ]
    prompt = prompt_template.replace("{{ENTITY_LIST_JSON}}", json.dumps(simplified_entities, ensure_ascii=False, indent=2))
    
    llm_response = call_llm(prompt, model_name="qwen-plus-latest")
    
    # å…³é”®ï¼šæˆ‘ä»¬æœŸæœ›çš„è¾“å‡ºæ˜¯åŒ…å« "clusters" é”®çš„å­—å…¸
    if llm_response and "clusters" in llm_response and isinstance(llm_response["clusters"], list):
        print("æˆåŠŸä»LLMè·å–èšç±»ç»“æœã€‚")
        return llm_response["clusters"]
    else:
        print("è­¦å‘Š: LLMè¿”å›çš„æ ¼å¼ä¸æ­£ç¡®æˆ–ä¸ºç©ºã€‚æ— æ³•è¿›è¡Œèšç±»ã€‚")
        print("LLMè¿”å›å†…å®¹:", llm_response)
        return None

def process_llm_clusters(original_entities: list, clusters: list):
    """
    æ ¹æ®LLMè¿”å›çš„èšç±»ç»“æœï¼Œç²¾ç¡®åœ°åˆå¹¶åŸå§‹å®ä½“åˆ—è¡¨ï¼Œç‰¹åˆ«æ˜¯chunk_idã€‚
    """
    if not clusters:
        print("æ²¡æœ‰èšç±»ä¿¡æ¯ï¼Œè¿”å›åŸå§‹å®ä½“åˆ—è¡¨ã€‚")
        return original_entities

    print("æ­£åœ¨æ ¹æ®èšç±»é…æ–¹ï¼Œåœ¨Pythonä¸­ç²¾ç¡®åˆå¹¶å®ä½“...")
    
    final_entities = []
    # åˆ›å»ºä¸€ä¸ªä» entity_text åˆ°å…¶å®Œæ•´å¯¹è±¡çš„æ˜ å°„ï¼Œæ”¯æŒä¸€ä¸ªtextå¯¹åº”å¤šä¸ªå¯¹è±¡ï¼ˆå› ä¸ºè¾“å…¥ä¸­å¯èƒ½æœ‰é‡å¤ï¼‰
    entity_map = defaultdict(list)
    for e in original_entities:
        entity_map[e["entity_text"]].append(e)

    processed_texts = set()

    for cluster in clusters:
        aliases = cluster.get("aliases", [])
        canonical_name = cluster.get("canonical_name")

        if not aliases or not canonical_name:
            continue
        
        # ç¡®ä¿è§„èŒƒåç§°æœ¬èº«ä¹Ÿåœ¨åˆ«ååˆ—è¡¨ä¸­ï¼Œä¾¿äºå¤„ç†
        if canonical_name not in aliases:
            aliases.append(canonical_name)

        merged_chunk_ids = set()
        
        # æ‰¾åˆ°è§„èŒƒåç§°å¯¹åº”çš„é‚£ä¸ªå®ä½“ï¼Œä»¥å®ƒçš„ç±»å‹å’Œæè¿°ä¸ºå‡†
        # ä¼˜å…ˆä½¿ç”¨ä¸canonical_nameå®Œå…¨åŒ¹é…çš„å®ä½“ä½œä¸ºæ¨¡æ¿
        canonical_entity_template = entity_map.get(canonical_name, [None])[0]
        if not canonical_entity_template:
            # å¦‚æœLLMç”Ÿæˆçš„è§„èŒƒåä¸åœ¨åŸæ–‡ä¸­ï¼Œå°±ç”¨åˆ«ååˆ—è¡¨é‡Œçš„ç¬¬ä¸€ä¸ª
            canonical_entity_template = entity_map.get(aliases[0], [None])[0]
            if not canonical_entity_template: continue # æç«¯æƒ…å†µï¼Œè·³è¿‡

        for alias in aliases:
            # ä¸€ä¸ªåˆ«åå¯èƒ½å¯¹åº”å¤šä¸ªå®ä½“å¯¹è±¡ï¼ˆä¾‹å¦‚ "ç‚¼é“ç³»ç»Ÿ" åœ¨åŸæ–‡å‡ºç°å¤šæ¬¡ï¼‰
            if alias in entity_map:
                for entity_obj in entity_map[alias]:
                    merged_chunk_ids.update(entity_obj["chunk_id"])
                processed_texts.add(alias)
        
        # åˆ›å»ºåˆå¹¶åçš„æ–°å®ä½“
        new_entity = {
            "entity_text": canonical_name,
            "entity_type": canonical_entity_template["entity_type"],
            "entity_description": canonical_entity_template["entity_description"],
            "chunk_id": sorted(list(merged_chunk_ids)), # åˆå¹¶å¹¶æ’åºchunk_id
            "aliases": aliases 
        }
        final_entities.append(new_entity)

    # æ·»åŠ é‚£äº›æœªè¢«èšç±»çš„ç‹¬ç«‹å®ä½“
    for entity_text, entity_objects in entity_map.items():
        if entity_text not in processed_texts:
            # å³ä½¿æ˜¯ç‹¬ç«‹å®ä½“ï¼Œä¹Ÿå¯èƒ½åœ¨åŸæ–‡å‡ºç°å¤šæ¬¡ï¼Œéœ€è¦åˆå¹¶å…¶chunk_id
            merged_chunk_ids = set()
            for entity_obj in entity_objects:
                merged_chunk_ids.update(entity_obj["chunk_id"])
            
            # ä½¿ç”¨ç¬¬ä¸€ä¸ªå¯¹è±¡ä½œä¸ºæ¨¡æ¿
            template_obj = entity_objects[0].copy()  # åˆ›å»ºå‰¯æœ¬é¿å…ä¿®æ”¹åŸå¯¹è±¡
            template_obj["chunk_id"] = sorted(list(merged_chunk_ids))
            final_entities.append(template_obj)

    print(f"å®ä½“åˆå¹¶å®Œæˆã€‚åŸå§‹å®ä½“æ•°: {len(original_entities)}, åˆå¹¶åå®ä½“æ•°: {len(final_entities)}")
    return final_entities


def run_disambiguate_on_all_files():
    """å¯¹æ•´ä¸ªç›®å½•ä¸‹çš„æ‰€æœ‰NERæ–‡ä»¶è¿›è¡Œå®ä½“æ¶ˆæ­§ï¼Œåˆå¹¶ä¸ºä¸€ä¸ªJSONæ–‡ä»¶"""
    print("ğŸ”„ å¼€å§‹å¤„ç†æ•´ä¸ªç›®å½•çš„å®ä½“æ¶ˆæ­§...")
    
    # æ”¶é›†æ‰€æœ‰æ–‡ä»¶çš„å®ä½“
    all_entities = []
    processed_files = []
    
    # éå†æ‰€æœ‰NERè¾“å‡ºæ–‡ä»¶
    for filename in os.listdir(config.NER_OUTPUT_DIR):
        if filename.endswith(".json"):
            input_path = os.path.join(config.NER_OUTPUT_DIR, filename)
            print(f"ğŸ“„ æ­£åœ¨åŠ è½½æ–‡ä»¶: {filename}")
            
            entities = load_json(input_path)
            if entities:
                all_entities.extend(entities)
                processed_files.append(filename)
                print(f"âœ… ä» {filename} åŠ è½½äº† {len(entities)} ä¸ªå®ä½“")
            else:
                print(f"âš ï¸  æ–‡ä»¶ {filename} å®ä½“æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
    
    if not all_entities:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å®ä½“æ•°æ®")
        return
    
    print(f"ğŸ“Š æ€»å…±æ”¶é›†åˆ° {len(all_entities)} ä¸ªå®ä½“ï¼Œæ¥è‡ª {len(processed_files)} ä¸ªæ–‡ä»¶")
    
    # 1. è°ƒç”¨LLMè·å–èšç±»"é…æ–¹"
    print("ğŸ¤– æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œå®ä½“èšç±»...")
    clusters_recipe = disambiguate_entities_with_llm(all_entities)
    
    # 2. åœ¨Pythonä¸­æ ¹æ®é…æ–¹è¿›è¡Œå¤„ç†
    print("ğŸ”§ æ­£åœ¨æ ¹æ®èšç±»ç»“æœåˆå¹¶å®ä½“...")
    final_disambiguated_entities = process_llm_clusters(all_entities, clusters_recipe)
    
    # 3. ä¿å­˜åˆå¹¶åçš„ç»“æœåˆ°å•ä¸ªæ–‡ä»¶
    output_filepath = os.path.join(config.NER_PRO_OUTPUT_DIR, "all_entities_disambiguated.json")
    save_json(final_disambiguated_entities, output_filepath)
    
    print(f"âœ… å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   ğŸ“„ å¤„ç†æ–‡ä»¶æ•°: {len(processed_files)}")
    print(f"   ğŸ·ï¸  åŸå§‹å®ä½“æ•°: {len(all_entities)}")
    print(f"   ğŸ”— æ¶ˆæ­§åå®ä½“æ•°: {len(final_disambiguated_entities)}")
    print(f"   ğŸ’¾ ç»“æœä¿å­˜åˆ°: {output_filepath}")

if __name__ == "__main__":
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config.NER_PRO_OUTPUT_DIR, exist_ok=True)
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶å¹¶åˆå¹¶ç»“æœ
    run_disambiguate_on_all_files()
    
    print("\nğŸ‰ æ‰€æœ‰NERæ–‡ä»¶çš„å®ä½“æ¶ˆæ­§å¤„ç†å®Œæˆï¼")