import os
import config
from utils import call_llm, save_json, load_prompt, load_json
import tqdm
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading
from functools import partial

# çº¿ç¨‹é”ï¼Œç”¨äºä¿æŠ¤å…±äº«èµ„æº
result_lock = threading.Lock()

def process_single_chunk(chunk_id, chunk_text, prompt_template, file_prefix):
    """å¤„ç†å•ä¸ªchunkçš„NERä»»åŠ¡"""
    try:
        prompt = prompt_template.replace("{{ENTITY_TYPES}}", str(config.ENTITY_TYPES))
        prompt = prompt.replace("{{TEXT_CONTENT}}", chunk_text)
        chunk_ner = call_llm(prompt, model_name="qwen-max")
        
        if chunk_ner and isinstance(chunk_ner, list):
            for entity in chunk_ner:
                # ä¸ºchunk_idæ·»åŠ æ–‡ä»¶åå‰ç¼€ï¼Œæ ¼å¼ï¼šæ–‡ä»¶å_chunk_id
                entity['chunk_id'] = [f"{file_prefix}_{chunk_id}"]
            return chunk_ner
        return []
    except Exception as e:
        print(f"å¤„ç†chunk {chunk_id} æ—¶å‡ºé”™: {e}")
        return []

def run_ner_on_file(chunk_filepath, max_workers=4):
    """å¯¹å•ä¸ªåˆ†å—æ–‡ä»¶è¿›è¡Œå‘½åå®ä½“è¯†åˆ«ï¼ˆå¤šçº¿ç¨‹ç‰ˆæœ¬ï¼‰"""
    print(f"ğŸ”„ æ­£åœ¨å¤„ç†åˆ†å—æ–‡ä»¶: {chunk_filepath}")
    
    # 1. åŠ è½½åˆ†å—æ•°æ®
    chunks = load_json(chunk_filepath)
    if not chunks:
        print("âš ï¸ åˆ†å—æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ã€‚")
        return

    ner_results = []
    prompt_template = load_prompt(os.path.join(config.BASE_DIR, "prompts", "ner_prompt.txt"))

    # è·å–æ–‡ä»¶åå‰ç¼€ï¼ˆå»æ‰.jsonæ‰©å±•åï¼‰
    file_prefix = os.path.basename(chunk_filepath).replace('.json', '')
    
    # 2. ä½¿ç”¨å¤šçº¿ç¨‹å¯¹æ¯ä¸ªchunkè¿›è¡ŒNER
    print(f"ğŸ“Š å¼€å§‹å¤„ç† {len(chunks)} ä¸ªchunksï¼Œä½¿ç”¨ {max_workers} ä¸ªçº¿ç¨‹")
    
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_chunk = {
            executor.submit(process_single_chunk, chunk_id, chunk_text, prompt_template, file_prefix): chunk_id
            for chunk_id, chunk_text in chunks.items()
        }
        
        # ä½¿ç”¨tqdmæ˜¾ç¤ºè¿›åº¦
        with tqdm.tqdm(total=len(chunks), desc="å¤„ç†chunks") as pbar:
            for future in as_completed(future_to_chunk):
                chunk_id = future_to_chunk[future]
                try:
                    chunk_ner = future.result()
                    if chunk_ner:
                        with result_lock:
                            ner_results.extend(chunk_ner)
                except Exception as e:
                    print(f"âŒ å¤„ç†chunk {chunk_id} æ—¶å‡ºé”™: {e}")
                finally:
                    pbar.update(1)


    # 3. ä¿å­˜æ‰€æœ‰chunksçš„NERç»“æœ
    if ner_results:
        filename = os.path.basename(chunk_filepath)
        output_filepath = os.path.join(config.NER_OUTPUT_DIR, filename)
        save_json(ner_results, output_filepath)
        print(f"âœ… NERç»“æœå·²ä¿å­˜åˆ°: {output_filepath}ï¼Œå…±å¤„ç† {len(ner_results)} ä¸ªå®ä½“")
    else:
        print("âš ï¸ æœªèƒ½ä»APIè·å–NERç»“æœã€‚")

def process_file_wrapper(chunk_path, max_workers_per_file=4):
    """æ–‡ä»¶å¤„ç†çš„åŒ…è£…å‡½æ•°ï¼Œç”¨äºå¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶"""
    try:
        run_ner_on_file(chunk_path, max_workers_per_file)
        return f"âœ… æˆåŠŸå¤„ç†: {os.path.basename(chunk_path)}"
    except Exception as e:
        error_msg = f"âŒ å¤„ç†æ–‡ä»¶ {os.path.basename(chunk_path)} æ—¶å‡ºé”™: {e}"
        print(error_msg)
        return error_msg

if __name__ == "__main__":
    # å¤šçº¿ç¨‹é…ç½®
    MAX_FILE_WORKERS = 2  # åŒæ—¶å¤„ç†çš„æ–‡ä»¶æ•°é‡
    MAX_CHUNK_WORKERS = 4  # æ¯ä¸ªæ–‡ä»¶å†…åŒæ—¶å¤„ç†çš„chunkæ•°é‡
    
    print("ğŸš€ å¼€å§‹å¤šçº¿ç¨‹NERå¤„ç†...")
    print(f"ğŸ“‹ é…ç½®: æ–‡ä»¶å¹¶å‘æ•°={MAX_FILE_WORKERS}, æ¯æ–‡ä»¶chunkå¹¶å‘æ•°={MAX_CHUNK_WORKERS}")
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
    os.makedirs(config.NER_OUTPUT_DIR, exist_ok=True)

    # è·å–æ‰€æœ‰éœ€è¦å¤„ç†çš„æ–‡ä»¶
    chunk_files = []
    for filename in os.listdir(config.CHUNK_OUTPUT_DIR):
        if filename.endswith(".json"):
            chunk_path = os.path.join(config.CHUNK_OUTPUT_DIR, filename)
            chunk_files.append(chunk_path)
    
    if not chunk_files:
        print("âš ï¸ æœªæ‰¾åˆ°éœ€è¦å¤„ç†çš„åˆ†å—æ–‡ä»¶")
        exit(1)
    
    print(f"ğŸ“ æ‰¾åˆ° {len(chunk_files)} ä¸ªåˆ†å—æ–‡ä»¶å¾…å¤„ç†")
    
    # ä½¿ç”¨å¤šçº¿ç¨‹å¤„ç†æ–‡ä»¶
    with ThreadPoolExecutor(max_workers=MAX_FILE_WORKERS) as executor:
        # åˆ›å»ºéƒ¨åˆ†å‡½æ•°ï¼Œå›ºå®šchunk workeræ•°é‡
        process_func = partial(process_file_wrapper, max_workers_per_file=MAX_CHUNK_WORKERS)
        
        # æäº¤æ‰€æœ‰æ–‡ä»¶å¤„ç†ä»»åŠ¡
        future_to_file = {
            executor.submit(process_func, chunk_path): chunk_path
            for chunk_path in chunk_files
        }
        
        # æ˜¾ç¤ºæ€»ä½“è¿›åº¦
        results = []
        with tqdm.tqdm(total=len(chunk_files), desc="å¤„ç†æ–‡ä»¶") as pbar:
            for future in as_completed(future_to_file):
                file_path = future_to_file[future]
                try:
                    result = future.result()
                    results.append(result)
                except Exception as e:
                    error_msg = f"âŒ å¤„ç†æ–‡ä»¶ {os.path.basename(file_path)} æ—¶å‡ºç°å¼‚å¸¸: {e}"
                    print(error_msg)
                    results.append(error_msg)
                finally:
                    pbar.update(1)
    
    # è¾“å‡ºå¤„ç†ç»“æœç»Ÿè®¡
    success_count = sum(1 for r in results if "âœ…" in r)
    error_count = len(results) - success_count
    
    print("\n" + "="*50)
    print("ğŸ‰ æ‰€æœ‰åˆ†å—æ–‡ä»¶çš„å‘½åå®ä½“è¯†åˆ«å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {error_count} ä¸ª")
    if error_count > 0:
        print("âŒ å¤±è´¥çš„æ–‡ä»¶:")
        for result in results:
            if "âŒ" in result:
                print(f"   {result}")
    print("="*50)